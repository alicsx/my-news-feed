import google.generativeai as genai
import os
import re
import time
import json
import logging
import pandas as pd
import pandas_ta as ta
import requests
from datetime import datetime, timedelta, UTC
import aiohttp
import asyncio
from typing import Dict, List, Optional, Tuple
import concurrent.futures

# =================================================================================
# --- Ø¨Ø®Ø´ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ---
# =================================================================================

# Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ API
google_api_key = os.getenv("GOOGLE_API_KEY")
TWELVEDATA_API_KEY = os.getenv("TWELVEDATA_API_KEY")
CLOUDFLARE_AI_API_KEY = os.getenv("CLOUDFLARE_AI_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not all([google_api_key, TWELVEDATA_API_KEY]):
    raise ValueError("Ù„Ø·ÙØ§Ù‹ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ API Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯: GOOGLE_API_KEY, TWELVEDATA_API_KEY")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ…
HIGH_TIMEFRAME = "4h"
LOW_TIMEFRAME = "1h"
CANDLES_TO_FETCH = 300
CURRENCY_PAIRS_TO_ANALYZE = [
    "EUR/USD", "GBP/USD", "USD/JPY", "USD/CHF", "AUD/USD",
    "GBP/JPY", "EUR/JPY", "AUD/JPY", "NZD/USD", "USD/CAD"
]

CACHE_FILE = "signal_cache.json"
USAGE_TRACKER_FILE = "api_usage_tracker.json"
LOG_FILE = "trading_log.log"

# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±
GEMINI_MODEL = 'gemini-2.5-flash'

# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Cloudflare Ø¨Ø§ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±
CLOUDFLARE_MODELS = [
    "@cf/meta/llama-4-scout-17b-16e-instruct",  # Llama 4
    "@cf/google/gemma-3-12b-it",               # Gemma 3
    "@cf/mistralai/mistral-small-3.1-24b-instruct",  # Mistral
    "@cf/qwen/qwq-32b",                        # Qwen
    "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b"  # DeepSeek
]

# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Groq Ø¨Ø§ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±
GROQ_MODELS = [
    "llama-3.3-70b-versatile",                 # Llama 3.3
    "qwen/qwen3-32b",                          # Qwen 3
    "meta-llama/llama-4-maverick-17b-128e-instruct",  # Llama 4 Maverick
    "llama-3.1-8b-instant",                    # Llama 3.1
    "mixtral-8x7b-32768"                       # Mixtral
]

# Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ API
API_DAILY_LIMITS = {
    "google_gemini": 1500,
    "cloudflare": 10000,
    "groq": 10000
}

# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# =================================================================================
# --- Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ØµØ±Ù API Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ØªÙ†ÙˆØ¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ---
# =================================================================================

class SmartAPIManager:
    def __init__(self, usage_file: str):
        self.usage_file = usage_file
        self.usage_data = self.load_usage_data()
        self.available_models = self.initialize_available_models()
        self.failed_models = set()

    def load_usage_data(self) -> Dict:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±Ù API"""
        try:
            if os.path.exists(self.usage_file):
                with open(self.usage_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return self.check_and_reset_daily_usage(data)
            return self.initialize_usage_data()
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±Ù API: {e}")
            return self.initialize_usage_data()

    def initialize_usage_data(self) -> Dict:
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±Ù"""
        today = datetime.now(UTC).date().isoformat()
        return {
            "last_reset_date": today,
            "providers": {
                "google_gemini": {"used_today": 0, "limit": API_DAILY_LIMITS["google_gemini"]},
                "cloudflare": {"used_today": 0, "limit": API_DAILY_LIMITS["cloudflare"]},
                "groq": {"used_today": 0, "limit": API_DAILY_LIMITS["groq"]}
            }
        }

    def check_and_reset_daily_usage(self, data: Dict) -> Dict:
        """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø±ÛŒØ³Øª Ù…ØµØ±Ù Ø±ÙˆØ²Ø§Ù†Ù‡"""
        today = datetime.now(UTC).date().isoformat()
        last_reset = data.get("last_reset_date", "")
        
        if last_reset != today:
            for provider in data["providers"]:
                data["providers"][provider]["used_today"] = 0
            data["last_reset_date"] = today
            self.save_usage_data(data)
            logging.info("âœ… Ù…ØµØ±Ù Ø±ÙˆØ²Ø§Ù†Ù‡ APIÙ‡Ø§ Ø±ÛŒØ³Øª Ø´Ø¯")
        return data

    def save_usage_data(self, data: Dict = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±Ù"""
        if data is None:
            data = self.usage_data
        try:
            with open(self.usage_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±Ù API: {e}")

    def initialize_available_models(self) -> Dict:
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        return {
            "google_gemini": [GEMINI_MODEL],
            "cloudflare": CLOUDFLARE_MODELS.copy(),
            "groq": GROQ_MODELS.copy()
        }

    def can_use_provider(self, provider: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ú©Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² provider"""
        if provider not in self.usage_data["providers"]:
            return False
        provider_data = self.usage_data["providers"][provider]
        remaining = provider_data["limit"] - provider_data["used_today"]
        return remaining > 0

    def get_available_models_count(self, provider: str) -> int:
        """ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø±Ø§ÛŒ ÛŒÚ© provider"""
        if not self.can_use_provider(provider):
            return 0
        provider_data = self.usage_data["providers"][provider]
        remaining = provider_data["limit"] - provider_data["used_today"]
        available_models = len(self.available_models[provider])
        return min(remaining, available_models)

    def mark_model_failed(self, provider: str, model_name: str):
        """Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯Ù‡"""
        self.failed_models.add((provider, model_name))
        logging.warning(f"âŒ Ù…Ø¯Ù„ {provider}/{model_name} Ø¨Ù‡ Ù„ÛŒØ³Øª Ø´Ú©Ø³Øªâ€ŒØ®ÙˆØ±Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")

    def is_model_failed(self, provider: str, model_name: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ Ù…Ø¯Ù„ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯Ù‡ Ø§Ø³Øª"""
        return (provider, model_name) in self.failed_models

    def select_diverse_models(self, target_total: int = 5, min_required: int = 3) -> List[Tuple[str, str]]:
        """Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ Ø§Ø² providerÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        selected_models = []

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¸Ø±ÙÛŒØª Ù‡Ø± provider
        provider_capacity = {}
        for provider in ["google_gemini", "cloudflare", "groq"]:
            provider_capacity[provider] = self.get_available_models_count(provider)

        logging.info(f"ğŸ“Š Ø¸Ø±ÙÛŒØª providerÙ‡Ø§: Gemini={provider_capacity['google_gemini']}, "
                    f"Cloudflare={provider_capacity['cloudflare']}, Groq={provider_capacity['groq']}")

        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: Ø§Ù†ØªØ®Ø§Ø¨ Ù…ØªÙ†ÙˆØ¹ Ø§Ø² Ù‡Ù…Ù‡ providerÙ‡Ø§
        total_available = sum(provider_capacity.values())
        if total_available == 0:
            logging.error("âŒ Ù‡ÛŒÚ† providerÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª")
            return selected_models

        # ØªÙˆØ²ÛŒØ¹ Ù…ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† providerÙ‡Ø§
        providers_order = ["google_gemini", "cloudflare", "groq"]
        round_robin_index = 0
        remaining_target = min(target_total, total_available)

        while remaining_target > 0 and any(provider_capacity[p] > 0 for p in providers_order):
            current_provider = providers_order[round_robin_index % len(providers_order)]
            
            if provider_capacity[current_provider] > 0:
                # Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÙˆÙ„ÛŒÙ† Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø² Ø§ÛŒÙ† provider Ú©Ù‡ Ø´Ú©Ø³Øª Ù†Ø®ÙˆØ±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                for model_name in self.available_models[current_provider]:
                    if (current_provider, model_name) not in selected_models and not self.is_model_failed(current_provider, model_name):
                        selected_models.append((current_provider, model_name))
                        provider_capacity[current_provider] -= 1
                        remaining_target -= 1
                        break
            
            round_robin_index += 1
            
            # Ø§Ú¯Ø± Ø¨Ø¹Ø¯ Ø§Ø² ÛŒÚ© Ø¯ÙˆØ± Ú©Ø§Ù…Ù„ Ú†ÛŒØ²ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ø´Ø¯ØŒ break Ú©Ù†
            if round_robin_index > len(providers_order) * 2:  # Ø­Ø¯Ø§Ú©Ø«Ø± 2 Ø¯ÙˆØ± Ú†Ø±Ø®Ø´
                break

        # Ø§Ú¯Ø± Ø¨Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ù†Ø±Ø³ÛŒØ¯ÛŒÙ…ØŒ Fallback
        if len(selected_models) < min_required:
            logging.warning(f"âš ï¸ ÙÙ‚Ø· {len(selected_models)} Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯. ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Fallback...")
            # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
            additional_models = []
            for provider in providers_order:
                if self.can_use_provider(provider):
                    for model_name in self.available_models[provider]:
                        if (provider, model_name) not in selected_models and not self.is_model_failed(provider, model_name):
                            additional_models.append((provider, model_name))
                            if len(additional_models) >= (min_required - len(selected_models)):
                                break
                    if len(selected_models) + len(additional_models) >= min_required:
                        break
            
            selected_models.extend(additional_models)

        logging.info(f"ğŸ¯ {len(selected_models)} Ù…Ø¯Ù„ Ù…ØªÙ†ÙˆØ¹ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯: {selected_models}")
        return selected_models

    def record_api_usage(self, provider: str, count: int = 1):
        """Ø«Ø¨Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² API"""
        if provider in self.usage_data["providers"]:
            self.usage_data["providers"][provider]["used_today"] += count
            self.save_usage_data()

    def get_usage_summary(self) -> str:
        """Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ù…ØµØ±Ù"""
        summary = "ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù…ØµØ±Ù API:\n"
        for provider, data in self.usage_data["providers"].items():
            remaining = data["limit"] - data["used_today"]
            summary += f"  {provider}: {data['used_today']}/{data['limit']} ({remaining} Ø¨Ø§Ù‚ÛŒÙ…Ø§Ù†Ø¯Ù‡)\n"
        return summary

# =================================================================================
# --- Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ---
# =================================================================================

class AdvancedTechnicalAnalyzer:
    def __init__(self):
        self.indicators_config = {
            'trend': ['ema_21', 'ema_50', 'ema_200', 'adx_14'],
            'momentum': ['rsi_14', 'stoch_14_3_3', 'macd'],
            'volatility': ['bb_20_2', 'atr_14'],
            'ichimoku': True,
            'support_resistance': True,
            'candle_patterns': True
        }

    def calculate_advanced_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        if df is None or df.empty or len(df) < 100:
            return None

        try:
            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø±ÙˆÙ†Ø¯
            df.ta.ema(length=21, append=True)
            df.ta.ema(length=50, append=True)
            df.ta.ema(length=200, append=True)
            df.ta.adx(length=14, append=True)

            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù…ÙˆÙ…Ù†ØªÙˆÙ…
            df.ta.rsi(length=14, append=True)
            df.ta.stoch(append=True)
            df.ta.macd(append=True)

            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù†
            df.ta.bbands(length=20, std=2, append=True)
            df.ta.atr(length=14, append=True)

            # Ø§ÛŒÚ†ÛŒÙ…ÙˆÚ©Ùˆ
            df.ta.ichimoku(append=True)

            # Ø³Ø·ÙˆØ­ Ø­Ù…Ø§ÛŒØª Ùˆ Ù…Ù‚Ø§ÙˆÙ…Øª
            df['sup_1'] = df['low'].rolling(20).min().shift(1)
            df['res_1'] = df['high'].rolling(20).max().shift(1)
            df['sup_2'] = df['low'].rolling(50).min().shift(1)
            df['res_2'] = df['high'].rolling(50).max().shift(1)

            df.dropna(inplace=True)
            return df

        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§: {e}")
            return None

    def generate_technical_analysis(self, symbol: str, htf_df: pd.DataFrame, ltf_df: pd.DataFrame) -> Dict:
        if htf_df.empty or ltf_df.empty:
            return None

        last_htf = htf_df.iloc[-1]
        last_ltf = ltf_df.iloc[-1]

        # ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯
        htf_trend = self._analyze_trend(last_htf)
        ltf_trend = self._analyze_trend(last_ltf)

        # ØªØ­Ù„ÛŒÙ„ Ù…ÙˆÙ…Ù†ØªÙˆÙ…
        momentum = self._analyze_momentum(last_ltf)

        # ØªØ­Ù„ÛŒÙ„ Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ
        key_levels = self._analyze_key_levels(htf_df, ltf_df, last_ltf['close'])

        return {
            'symbol': symbol,
            'htf_trend': htf_trend,
            'ltf_trend': ltf_trend,
            'momentum': momentum,
            'key_levels': key_levels,
            'volatility': last_ltf.get('ATRr_14', 0),
            'timestamp': datetime.now(UTC).isoformat()
        }

    def _analyze_trend(self, data: pd.Series) -> Dict:
        ema_21 = data.get('EMA_21', 0)
        ema_50 = data.get('EMA_50', 0)
        ema_200 = data.get('EMA_200', 0)
        adx = data.get('ADX_14', 0)

        trend_direction = "ØµØ¹ÙˆØ¯ÛŒ" if ema_21 > ema_50 > ema_200 else "Ù†Ø²ÙˆÙ„ÛŒ" if ema_21 < ema_50 < ema_200 else "Ø®Ù†Ø«ÛŒ"
        trend_strength = "Ù‚ÙˆÛŒ" if adx > 25 else "Ø¶Ø¹ÛŒÙ" if adx < 20 else "Ù…ØªÙˆØ³Ø·"

        return {
            'direction': trend_direction,
            'strength': trend_strength,
            'adx': adx
        }

    def _analyze_momentum(self, data: pd.Series) -> Dict:
        rsi = data.get('RSI_14', 50)
        macd_hist = data.get('MACDh_12_26_9', 0)
        stoch_k = data.get('STOCHk_14_3_3', 50)

        rsi_signal = "Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯" if rsi > 70 else "Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´" if rsi < 30 else "Ø®Ù†Ø«ÛŒ"
        macd_signal = "ØµØ¹ÙˆØ¯ÛŒ" if macd_hist > 0 else "Ù†Ø²ÙˆÙ„ÛŒ"
        stoch_signal = "Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯" if stoch_k > 80 else "Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´" if stoch_k < 20 else "Ø®Ù†Ø«ÛŒ"

        return {
            'rsi': {'value': rsi, 'signal': rsi_signal},
            'macd': {'signal': macd_signal, 'histogram': macd_hist},
            'stochastic': {'value': stoch_k, 'signal': stoch_signal}
        }

    def _analyze_key_levels(self, htf_df: pd.DataFrame, ltf_df: pd.DataFrame, current_price: float) -> Dict:
        bb_upper = ltf_df.get('BBU_20_2.0', pd.Series([0])).iloc[-1]
        bb_lower = ltf_df.get('BBL_20_2.0', pd.Series([0])).iloc[-1]
        support_1 = ltf_df.get('sup_1', pd.Series([0])).iloc[-1]
        resistance_1 = ltf_df.get('res_1', pd.Series([0])).iloc[-1]
        support_2 = ltf_df.get('sup_2', pd.Series([0])).iloc[-1]
        resistance_2 = ltf_df.get('res_2', pd.Series([0])).iloc[-1]

        return {
            'support_1': support_1,
            'resistance_1': resistance_1,
            'support_2': support_2,
            'resistance_2': resistance_2,
            'bb_upper': bb_upper,
            'bb_lower': bb_lower
        }

# =================================================================================
# --- Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª AI Ø¨Ø§ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ ---
# =================================================================================

class EnhancedAIManager:
    def __init__(self, gemini_api_key: str, cloudflare_api_key: str, groq_api_key: str, api_manager: SmartAPIManager):
        self.gemini_api_key = gemini_api_key
        self.cloudflare_api_key = cloudflare_api_key
        self.groq_api_key = groq_api_key
        self.api_manager = api_manager
        genai.configure(api_key=gemini_api_key)

    async def get_enhanced_ai_analysis(self, symbol: str, technical_analysis: Dict) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ Û³ Ù…Ø¯Ù„ AI"""
        selected_models = self.api_manager.select_diverse_models(target_total=5, min_required=3)
        
        if len(selected_models) < 3:
            logging.error(f"âŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø­Ø¯Ø§Ù‚Ù„ Û³ Ù…Ø¯Ù„ AI Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ {symbol} Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯")
            return None

        logging.info(f"ğŸ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² {len(selected_models)} Ù…Ø¯Ù„ AI Ø¨Ø±Ø§ÛŒ {symbol}")

        tasks = []
        for provider, model_name in selected_models:
            task = self._get_single_analysis(symbol, technical_analysis, provider, model_name)
            tasks.append(task)

        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            valid_results = []
            failed_count = 0
            
            for i, (provider, model_name) in enumerate(selected_models):
                result = results[i]
                if isinstance(result, Exception):
                    logging.error(f"Ø®Ø·Ø§ Ø¯Ø± {provider}/{model_name} Ø¨Ø±Ø§ÛŒ {symbol}: {result}")
                    self.api_manager.mark_model_failed(provider, model_name)
                    failed_count += 1
                    self.api_manager.record_api_usage(provider)
                elif result is not None:
                    valid_results.append(result)
                    self.api_manager.record_api_usage(provider)
                else:
                    self.api_manager.record_api_usage(provider)

            logging.info(f"ğŸ“Š Ù†ØªØ§ÛŒØ¬: {len(valid_results)} Ù…ÙˆÙÙ‚, {failed_count} Ø´Ú©Ø³Øª")
            return self._combine_signals(symbol, valid_results, len(selected_models))

        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ AI Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    async def _get_single_analysis(self, symbol: str, technical_analysis: Dict, provider: str, model_name: str) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ ÛŒÚ© Ù…Ø¯Ù„ Ø®Ø§Øµ"""
        try:
            prompt = self._create_enhanced_prompt(symbol, technical_analysis)
            
            if provider == "google_gemini":
                return await self._get_gemini_analysis(symbol, prompt, model_name)
            elif provider == "cloudflare":
                return await self._get_cloudflare_analysis(symbol, prompt, model_name)
            elif provider == "groq":
                return await self._get_groq_analysis(symbol, prompt, model_name)
            else:
                return None
                
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ {provider}/{model_name} Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    def _create_enhanced_prompt(self, symbol: str, technical_analysis: Dict) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        current_price = 1.0850  # Ù…Ù‚Ø¯Ø§Ø± Ù†Ù…ÙˆÙ†Ù‡ - Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯
        
        return f"""
IMPORTANT: You MUST return ONLY valid JSON format. No other text, no explanations.

As a professional forex trading analyst, analyze this currency pair and provide trading signals:

SYMBOL: {symbol}
CURRENT PRICE: ~{current_price}

TECHNICAL ANALYSIS:
- HTF Trend (4H): {technical_analysis['htf_trend']['direction']} ({technical_analysis['htf_trend']['strength']})
- LTF Trend (1H): {technical_analysis['ltf_trend']['direction']}
- RSI: {technical_analysis['momentum']['rsi']['value']:.1f} ({technical_analysis['momentum']['rsi']['signal']})
- MACD: {technical_analysis['momentum']['macd']['signal']}
- Key Support: {technical_analysis['key_levels']['support_1']:.5f}
- Key Resistance: {technical_analysis['key_levels']['resistance_1']:.5f}

CALCULATE REALISTIC LEVELS based on current price ~{current_price} and technical structure.

RETURN ONLY THIS EXACT JSON FORMAT:
{{
  "SYMBOL": "{symbol}",
  "ACTION": "BUY",
  "CONFIDENCE": 7,
  "ENTRY_ZONE": "{current_price - 0.0010:.5f}-{current_price + 0.0005:.5f}",
  "STOP_LOSS": "{current_price - 0.0020:.5f}",
  "TAKE_PROFIT": "{current_price + 0.0030:.5f}",
  "RISK_REWARD_RATIO": "1.5",
  "ANALYSIS": "ØªØ­Ù„ÛŒÙ„ ÙÙ†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ùˆ Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ù…Ø«Ø¨Øª",
  "EXPIRATION_H": 4,
  "TRADE_RATIONALE": "Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§"
}}

Your response must be ONLY the JSON object.
"""

    async def _get_gemini_analysis(self, symbol: str, prompt: str, model_name: str) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Gemini"""
        try:
            model = genai.GenerativeModel(model_name)
            response = await asyncio.to_thread(
                model.generate_content,
                prompt,
                request_options={'timeout': 60}
            )
            return self._parse_ai_response(response.text, symbol, f"Gemini-{model_name}")
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Gemini Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    async def _get_cloudflare_analysis(self, symbol: str, prompt: str, model_name: str) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Cloudflare AI"""
        if not self.cloudflare_api_key:
            return None

        try:
            headers = {
                "Authorization": f"Bearer {self.cloudflare_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a forex trading expert. Return ONLY valid JSON format."
                    },
                    {"role": "user", "content": prompt}
                ],
                "stream": False
            }

            account_id = os.getenv("CLOUDFLARE_ACCOUNT_ID", "your_account_id")
            url = f"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model_name}"

            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=payload, timeout=60) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = ""
                        if "result" in data and "response" in data["result"]:
                            content = data["result"]["response"]
                        elif "response" in data:
                            content = data["response"]
                            
                        if content:
                            return self._parse_ai_response(content, symbol, f"Cloudflare-{model_name}")
            return None
            
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Cloudflare/{model_name} Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    async def _get_groq_analysis(self, symbol: str, prompt: str, model_name: str) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Groq API"""
        if not self.groq_api_key:
            return None

        try:
            headers = {
                "Authorization": f"Bearer {self.groq_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a forex trading expert. Return ONLY valid JSON format."
                    },
                    {"role": "user", "content": prompt}
                ],
                "model": model_name,
                "temperature": 0.1,
                "max_tokens": 500,
                "stream": False
            }

            url = "https://api.groq.com/openai/v1/chat/completions"

            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=payload, timeout=60) as response:
                    if response.status == 200:
                        data = await response.json()
                        if "choices" in data and len(data["choices"]) > 0:
                            content = data["choices"][0]["message"]["content"]
                            return self._parse_ai_response(content, symbol, f"Groq-{model_name}")
            return None
            
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Groq/{model_name} Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    def _parse_ai_response(self, response: str, symbol: str, ai_name: str) -> Optional[Dict]:
        """Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø® AI Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        try:
            cleaned_response = response.strip()
            
            # Ø­Ø°Ù markdown Ùˆ Ú©Ø¯ Ø¨Ù„Ø§Ú©
            cleaned_response = re.sub(r'```json\s*', '', cleaned_response)
            cleaned_response = re.sub(r'```\s*', '', cleaned_response)
            
            # ÛŒØ§ÙØªÙ† JSON
            json_match = re.search(r'\{.*\}', cleaned_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                signal_data = json.loads(json_str)
                
                if self._validate_signal_data(signal_data, symbol):
                    signal_data['ai_model'] = ai_name
                    signal_data['timestamp'] = datetime.now(UTC).isoformat()
                    
                    # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ
                    signal_data = self._validate_numeric_values(signal_data, symbol)
                    logging.info(f"âœ… {ai_name} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø±Ø§ÛŒ {symbol}: {signal_data.get('ACTION', 'HOLD')}")
                    return signal_data

            logging.warning(f"âŒ Ù¾Ø§Ø³Ø® {ai_name} Ø¨Ø±Ø§ÛŒ {symbol} ÙØ§Ù‚Ø¯ ÙØ±Ù…Øª JSON Ù…Ø¹ØªØ¨Ø±")
            return None
            
        except json.JSONDecodeError as e:
            logging.error(f"Ø®Ø·Ø§ÛŒ JSON Ø¯Ø± Ù¾Ø§Ø³Ø® {ai_name} Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø® {ai_name} Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    def _validate_signal_data(self, signal_data: Dict, symbol: str) -> bool:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„"""
        required_fields = ['SYMBOL', 'ACTION', 'CONFIDENCE']
        
        for field in required_fields:
            if field not in signal_data:
                logging.warning(f"ÙÛŒÙ„Ø¯ Ø¶Ø±ÙˆØ±ÛŒ {field} Ø¯Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„ {symbol} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return False

        action = signal_data['ACTION'].upper()
        if action not in ['BUY', 'SELL', 'HOLD']:
            logging.warning(f"ACTION Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ {symbol}: {action}")
            return False

        try:
            confidence = float(signal_data['CONFIDENCE'])
            if not (1 <= confidence <= 10):
                logging.warning(f"CONFIDENCE Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¨Ø±Ø§ÛŒ {symbol}: {confidence}")
                return False
        except (ValueError, TypeError):
            logging.warning(f"CONFIDENCE Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ {symbol}: {signal_data['CONFIDENCE']}")
            return False

        return True

    def _validate_numeric_values(self, signal_data: Dict, symbol: str) -> Dict:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ"""
        numeric_fields = ['ENTRY_ZONE', 'STOP_LOSS', 'TAKE_PROFIT', 'EXPIRATION_H', 'RISK_REWARD_RATIO']
        
        for field in numeric_fields:
            if field in signal_data:
                value = signal_data[field]
                if value is None or value == "null" or str(value).strip() == "":
                    # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
                    if field == 'EXPIRATION_H':
                        signal_data[field] = 4
                    elif field == 'RISK_REWARD_RATIO':
                        signal_data[field] = "1.5"
                    else:
                        signal_data[field] = "N/A"
        
        return signal_data

    def _combine_signals(self, symbol: str, valid_results: List[Dict], total_models: int) -> Optional[Dict]:
        """ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§"""
        if not valid_results:
            return None

        # Ø´Ù…Ø§Ø±Ø´ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
        action_counts = {}
        for result in valid_results:
            action = result['ACTION'].upper()
            action_counts[action] = action_counts.get(action, 0) + 1

        total_valid = len(valid_results)
        max_agreement = max(action_counts.values())

        # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ ØªÙˆØ§ÙÙ‚
        if max_agreement >= 3:
            agreement_type = 'STRONG_CONSENSUS'
        elif max_agreement == 2:
            agreement_type = 'MEDIUM_CONSENSUS'
        else:
            agreement_type = 'WEAK_CONSENSUS'

        # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø§Ú©Ø«Ø±ÛŒØª
        majority_action = max(action_counts, key=action_counts.get)
        agreeing_results = [r for r in valid_results if r['ACTION'].upper() == majority_action]

        # ØªØ±Ú©ÛŒØ¨ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ§ÙÙ‚
        combined = {
            'SYMBOL': symbol,
            'ACTION': majority_action,
            'AGREEMENT_LEVEL': max_agreement,
            'AGREEMENT_TYPE': agreement_type,
            'VALID_MODELS': total_valid,
            'TOTAL_MODELS': total_models,
            'timestamp': datetime.now(UTC).isoformat()
        }

        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø¹ØªÙ…Ø§Ø¯
        if agreeing_results:
            confidences = [float(r.get('CONFIDENCE', 5)) for r in agreeing_results]
            combined['CONFIDENCE'] = round(sum(confidences) / len(confidences), 1)

        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹ØªØ¨Ø±
        if agreeing_results:
            first_valid = agreeing_results[0]
            for field in ['ENTRY_ZONE', 'STOP_LOSS', 'TAKE_PROFIT', 'RISK_REWARD_RATIO', 'EXPIRATION_H', 'ANALYSIS']:
                if field in first_valid and first_valid[field] not in [None, "null", ""]:
                    combined[field] = first_valid[field]
                else:
                    # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
                    if field == 'ANALYSIS':
                        combined[field] = f"Ø³ÛŒÚ¯Ù†Ø§Ù„ {majority_action} Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙˆØ§ÙÙ‚ {max_agreement} Ø§Ø² {total_models} Ù…Ø¯Ù„ AI"
                    elif field == 'EXPIRATION_H':
                        combined[field] = 4
                    elif field == 'RISK_REWARD_RATIO':
                        combined[field] = "1.5"

        return combined

# =================================================================================
# --- Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ ØªØ­Ù„ÛŒÙ„Ú¯Ø± ÙØ§Ø±Ú©Ø³ ---
# =================================================================================

class ImprovedForexAnalyzer:
    def __init__(self):
        self.api_manager = SmartAPIManager(USAGE_TRACKER_FILE)
        self.technical_analyzer = AdvancedTechnicalAnalyzer()
        self.ai_manager = EnhancedAIManager(google_api_key, CLOUDFLARE_AI_API_KEY, GROQ_API_KEY, self.api_manager)

    async def analyze_pair(self, pair: str) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ ÛŒÚ© Ø¬ÙØª Ø§Ø±Ø²"""
        logging.info(f"ğŸ” Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ {pair}")
        try:
            logging.info(self.api_manager.get_usage_summary())

            # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±
            htf_df = await self.get_market_data(pair, HIGH_TIMEFRAME)
            ltf_df = await self.get_market_data(pair, LOW_TIMEFRAME)
            
            if htf_df is None or ltf_df is None:
                logging.warning(f"âš ï¸ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨Ø±Ø§ÛŒ {pair} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
                return None

            # ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
            htf_df_processed = self.technical_analyzer.calculate_advanced_indicators(htf_df)
            ltf_df_processed = self.technical_analyzer.calculate_advanced_indicators(ltf_df)
            
            if htf_df_processed is None or ltf_df_processed is None:
                logging.warning(f"âš ï¸ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø±Ø§ÛŒ {pair} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
                return None

            technical_analysis = self.technical_analyzer.generate_technical_analysis(
                pair, htf_df_processed, ltf_df_processed
            )
            
            if not technical_analysis:
                logging.warning(f"âš ï¸ ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø±Ø§ÛŒ {pair} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
                return None

            # ØªØ­Ù„ÛŒÙ„ AI
            ai_analysis = await self.ai_manager.get_enhanced_ai_analysis(pair, technical_analysis)
            
            if ai_analysis and ai_analysis.get('ACTION') != 'HOLD':
                logging.info(f"âœ… Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø±Ø§ÛŒ {pair}: {ai_analysis['ACTION']} "
                            f"(ØªÙˆØ§ÙÙ‚: {ai_analysis.get('AGREEMENT_LEVEL', 0)}/{ai_analysis.get('TOTAL_MODELS', 0)})")
                return ai_analysis
            
            logging.info(f"ğŸ” Ù‡ÛŒÚ† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø¨Ø±Ø§ÛŒ {pair}")
            return None
            
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ {pair}: {e}")
            return None

    async def get_market_data(self, symbol: str, interval: str) -> Optional[pd.DataFrame]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±"""
        try:
            url = f'https://api.twelvedata.com/time_series?symbol={symbol}&interval={interval}&outputsize={CANDLES_TO_FETCH}&apikey={TWELVEDATA_API_KEY}'
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=30) as response:
                    if response.status == 200:
                        data = await response.json()
                        if 'values' in data and data['values']:
                            df = pd.DataFrame(data['values'])
                            df = df.iloc[::-1].reset_index(drop=True)
                            
                            for col in ['open', 'high', 'low', 'close']:
                                if col in df.columns:
                                    df[col] = pd.to_numeric(df[col], errors='coerce')
                            
                            df = df.dropna()
                            return df
            return None
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    async def analyze_all_pairs(self, pairs: List[str]) -> List[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ù‡ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§"""
        logging.info(f"ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ {len(pairs)} Ø¬ÙØª Ø§Ø±Ø²")
        
        tasks = [self.analyze_pair(pair) for pair in pairs]
        results = await asyncio.gather(*tasks)
        
        valid_signals = [r for r in results if r is not None]
        logging.info(f"ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. {len(valid_signals)} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹ØªØ¨Ø±")
        return valid_signals

    def save_signals(self, signals: List[Dict]):
        """Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        if not signals:
            logging.info("ğŸ“ Ù‡ÛŒÚ† Ø³ÛŒÚ¯Ù†Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return

        strong_signals = []
        medium_signals = []
        weak_signals = []

        for signal in signals:
            agreement_type = signal.get('AGREEMENT_TYPE', '')
            if agreement_type == 'STRONG_CONSENSUS':
                strong_signals.append(signal)
            elif agreement_type == 'MEDIUM_CONSENSUS':
                medium_signals.append(signal)
            else:
                weak_signals.append(signal)

        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§
        try:
            if strong_signals:
                with open("strong_consensus_signals.json", 'w', encoding='utf-8') as f:
                    json.dump(strong_signals, f, indent=2, ensure_ascii=False)
                logging.info(f"ğŸ’¾ {len(strong_signals)} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚ÙˆÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            
            if medium_signals:
                with open("medium_consensus_signals.json", 'w', encoding='utf-8') as f:
                    json.dump(medium_signals, f, indent=2, ensure_ascii=False)
                logging.info(f"ğŸ’¾ {len(medium_signals)} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…ØªÙˆØ³Ø· Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            
            if weak_signals:
                with open("weak_consensus_signals.json", 'w', encoding='utf-8') as f:
                    json.dump(weak_signals, f, indent=2, ensure_ascii=False)
                logging.info(f"ğŸ’¾ {len(weak_signals)} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¶Ø¹ÛŒÙ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
                
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§: {e}")

# =================================================================================
# --- ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ---
# =================================================================================

async def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    logging.info("ğŸ¯ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ ÙØ§Ø±Ú©Ø³ (Improved AI Engine)")
    
    import argparse
    parser = argparse.ArgumentParser(description='Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ ÙØ§Ø±Ú©Ø³ Ø¨Ø§ AI')
    parser.add_argument("--pair", type=str, help="ØªØ­Ù„ÛŒÙ„ Ø¬ÙØª Ø§Ø±Ø² Ù…Ø´Ø®Øµ")
    parser.add_argument("--all", action="store_true", help="ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ù‡ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§")
    parser.add_argument("--pairs", type=str, help="ØªØ­Ù„ÛŒÙ„ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡")
    
    args = parser.parse_args()

    if args.pair:
        pairs_to_analyze = [args.pair]
    elif args.pairs:
        pairs_to_analyze = [p.strip() for p in args.pairs.split(',')]
    elif args.all:
        pairs_to_analyze = CURRENCY_PAIRS_TO_ANALYZE
    else:
        pairs_to_analyze = CURRENCY_PAIRS_TO_ANALYZE[:2]
        logging.info(f"ğŸ” Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² 2 Ø¬ÙØª Ø§Ø±Ø² Ø§ØµÙ„ÛŒ")

    logging.info(f"ğŸ¯ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„: {', '.join(pairs_to_analyze)}")

    analyzer = ImprovedForexAnalyzer()
    signals = await analyzer.analyze_all_pairs(pairs_to_analyze)

    # Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
    analyzer.save_signals(signals)

    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    logging.info("ğŸ“ˆ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
    
    strong_count = len([s for s in signals if s.get('AGREEMENT_TYPE') == 'STRONG_CONSENSUS'])
    medium_count = len([s for s in signals if s.get('AGREEMENT_TYPE') == 'MEDIUM_CONSENSUS'])
    weak_count = len([s for s in signals if s.get('AGREEMENT_TYPE') == 'WEAK_CONSENSUS'])
    
    logging.info(f"ğŸ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ: {strong_count}")
    logging.info(f"ğŸ“Š Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ³Ø·: {medium_count}")
    logging.info(f"ğŸ“ˆ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ: {weak_count}")
    
    for signal in signals:
        action_icon = "ğŸŸ¢" if signal['ACTION'] == 'BUY' else "ğŸ”´" if signal['ACTION'] == 'SELL' else "âšª"
        logging.info(f"  {action_icon} {signal['SYMBOL']}: {signal['ACTION']} (Ø§Ø¹ØªÙ…Ø§Ø¯: {signal.get('CONFIDENCE', 0)}/10)")

    # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ù†Ù‡Ø§ÛŒÛŒ API
    analyzer.api_manager.save_usage_data()
    logging.info(analyzer.api_manager.get_usage_summary())
    
    logging.info("ğŸ Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…")

if __name__ == "__main__":
    asyncio.run(main())
