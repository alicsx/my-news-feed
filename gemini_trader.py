import google.generativeai as genai
import os
import re
import time
import json
import logging
import pandas as pd
import pandas_ta as ta
import requests
from datetime import datetime, timedelta, UTC
import aiohttp
import asyncio
from typing import Dict, List, Optional, Tuple
import concurrent.futures
import sys

# =================================================================================
# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ GitHub Actions ---
# =================================================================================

# Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ API - Ø¨Ø±Ø§ÛŒ GitHub Actions Ø§Ø² Secrets Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
google_api_key = os.getenv("GOOGLE_API_KEY")
TWELVEDATA_API_KEY = os.getenv("TWELVEDATA_API_KEY")
CLOUDFLARE_AI_API_KEY = os.getenv("CLOUDFLARE_AI_API_KEY")

if not all([google_api_key, TWELVEDATA_API_KEY]):
    raise ValueError("Ù„Ø·ÙØ§Ù‹ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ API Ø±Ø§ Ø¯Ø± GitHub Secrets ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ GitHub Actions
HIGH_TIMEFRAME = "4h"
LOW_TIMEFRAME = "1h"
CANDLES_TO_FETCH = 200  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø¯Ø± API calls
CURRENCY_PAIRS_TO_ANALYZE = [
    "EUR/USD", "GBP/USD", "USD/JPY", "USD/CHF", "AUD/USD",
    "GBP/JPY", "EUR/JPY", "AUD/JPY", "NZD/USD", "USD/CAD"
]

CACHE_FILE = "signal_cache.json"
CACHE_DURATION_HOURS = 2
LOG_FILE = "trading_log.log"

# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI Ù¾ÛŒØ´Ø±ÙØªÙ‡
GEMINI_MODEL = 'gemini-2.5-flash'
IMPROVED_CLOUDFLARE_MODELS = [
    "@cf/meta/llama-3-8b-instruct",  # Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ
    "@cf/qwen/qwen1.5-14b-chat-awq",  # Ù…Ø¯Ù„ fallback
    "@cf/meta/llama-2-7b-chat-fp16"   # Ù…Ø¯Ù„ reserve
]

# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

class AsyncRateLimiter:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù†Ø±Ø® Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§"""
    def __init__(self, rate_limit: int, period: int):
        self.rate_limit = rate_limit
        self.period = period
        self.request_timestamps = []
        self._lock = asyncio.Lock()

    async def __aenter__(self):
        async with self._lock:
            while True:
                now = time.time()
                self.request_timestamps = [
                    t for t in self.request_timestamps if now - t < self.period
                ]
                if len(self.request_timestamps) < self.rate_limit:
                    self.request_timestamps.append(now)
                    break
                oldest_request_time = self.request_timestamps[0]
                sleep_duration = self.period - (now - oldest_request_time)
                if sleep_duration > 0:
                    await asyncio.sleep(sleep_duration)

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass

# =================================================================================
# --- Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ ---
# =================================================================================

class SmartCacheManager:
    def __init__(self, cache_file: str, cache_duration_hours: int):
        self.cache_file = cache_file
        self.cache_duration_hours = cache_duration_hours
        self.cache = self.load_cache()
        
    def load_cache(self) -> Dict:
        if not os.path.exists(self.cache_file):
            return {}
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
                return self.clean_old_cache(cache)
        except (json.JSONDecodeError, IOError) as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø´: {e}")
            return {}
    
    def clean_old_cache(self, cache: Dict) -> Dict:
        cleaned_cache = {}
        current_time = datetime.now(UTC)
        
        for pair, cache_data in cache.items():
            if isinstance(cache_data, str):
                last_signal_time = datetime.fromisoformat(cache_data)
                if current_time - last_signal_time < timedelta(hours=self.cache_duration_hours):
                    cleaned_cache[pair] = cache_data
            elif isinstance(cache_data, dict):
                signal_time = datetime.fromisoformat(cache_data.get('timestamp', ''))
                if current_time - signal_time < timedelta(hours=self.cache_duration_hours):
                    cleaned_cache[pair] = cache_data
                    
        return cleaned_cache
    
    def is_pair_on_cooldown(self, pair: str) -> bool:
        if pair not in self.cache:
            return False
            
        cache_data = self.cache[pair]
        if isinstance(cache_data, str):
            last_signal_time = datetime.fromisoformat(cache_data)
        else:
            last_signal_time = datetime.fromisoformat(cache_data.get('timestamp', ''))
            
        if datetime.now(UTC) - last_signal_time < timedelta(hours=self.cache_duration_hours):
            logging.info(f"Ø¬ÙØª Ø§Ø±Ø² {pair} Ø¯Ø± Ø¯ÙˆØ±Ù‡ Ø§Ø³ØªØ±Ø§Ø­Øª Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯")
            return True
        return False
    
    def update_cache(self, pair: str, signal_data: Dict = None):
        self.cache[pair] = {
            'timestamp': datetime.now(UTC).isoformat(),
            'signal': signal_data or {}
        }
        self.save_cache()
    
    def save_cache(self):
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, indent=4, ensure_ascii=False)
        except IOError as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ú©Ø´: {e}")

# =================================================================================
# --- Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ± ---
# =================================================================================

class EnhancedTechnicalAnalyzer:
    def __init__(self):
        self.indicators_config = {
            'trend': ['ema_8', 'ema_21', 'ema_50', 'ema_200', 'adx_14', 'psar'],
            'momentum': ['rsi_14', 'stoch_14_3_3', 'macd', 'cci_20', 'williams_14'],
            'volatility': ['bb_20_2', 'atr_14', 'kc_20'],
            'volume': ['obv', 'volume_sma_20', 'mfi_14'],
            'ichimoku': True,
            'support_resistance': True,
            'candle_patterns': True,
            'pivot_points': True
        }

    def calculate_enhanced_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        if df is None or df.empty or len(df) < 100:
            return None
            
        try:
            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø±ÙˆÙ†Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡
            df.ta.ema(length=8, append=True)
            df.ta.ema(length=21, append=True)
            df.ta.ema(length=50, append=True)
            df.ta.ema(length=200, append=True)
            df.ta.adx(length=14, append=True)
            df.ta.psar(append=True)
            
            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡
            df.ta.rsi(length=14, append=True)
            df.ta.stoch(append=True)
            df.ta.macd(append=True)
            df.ta.cci(length=20, append=True)
            df.ta.willr(length=14, append=True)
            
            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù†
            df.ta.bbands(length=20, std=2, append=True)
            df.ta.atr(length=14, append=True)
            df.ta.kc(length=20, append=True)
            
            # Ø­Ø¬Ù… Ù¾ÛŒØ´Ø±ÙØªÙ‡
            if 'volume' in df.columns and not df['volume'].isnull().all():
                df.ta.obv(append=True)
                df['volume_sma_20'] = df['volume'].rolling(20).mean()
                df.ta.mfi(length=14, append=True)
            
            # Ø§ÛŒÚ†ÛŒÙ…ÙˆÚ©Ùˆ
            df.ta.ichimoku(append=True)
            
            # Ø³Ø·ÙˆØ­ Ø­Ù…Ø§ÛŒØª Ùˆ Ù…Ù‚Ø§ÙˆÙ…Øª Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ©
            df['sup_1'] = df['low'].rolling(20).min().shift(1)
            df['res_1'] = df['high'].rolling(20).max().shift(1)
            df['sup_2'] = df['low'].rolling(50).min().shift(1)
            df['res_2'] = df['high'].rolling(50).max().shift(1)
            
            # Ù¾ÛŒÙˆØª Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§
            df = self.calculate_pivot_points(df)
            
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„ Ø§Ø³ØªÛŒÚ©
            if self.indicators_config['candle_patterns']:
                enhanced_patterns = ['doji', 'hammer', 'engulfing', 'harami', 'morningstar', 
                                   'eveningstar', 'piercing', 'darkcloudcover']
                for pattern in enhanced_patterns:
                    try:
                        df.ta.cdl_pattern(name=pattern, append=True)
                    except Exception as e:
                        continue
            
            df.dropna(inplace=True)
            return df
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡: {e}")
            return None

    def calculate_pivot_points(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒÙˆØª Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯"""
        if len(df) < 2:
            return df
            
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ² Ù‚Ø¨Ù„ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÙˆØª
            prev_high = df['high'].shift(1)
            prev_low = df['low'].shift(1)
            prev_close = df['close'].shift(1)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒÙˆØª Ø§ØµÙ„ÛŒ
            df['pivot'] = (prev_high + prev_low + prev_close) / 3
            df['r1'] = 2 * df['pivot'] - prev_low
            df['s1'] = 2 * df['pivot'] - prev_high
            df['r2'] = df['pivot'] + (prev_high - prev_low)
            df['s2'] = df['pivot'] - (prev_high - prev_low)
            df['r3'] = df['pivot'] + 2 * (prev_high - prev_low)
            df['s3'] = df['pivot'] - 2 * (prev_high - prev_low)
            
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒÙˆØª Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§: {e}")
            
        return df

    def generate_enhanced_analysis(self, symbol: str, htf_df: pd.DataFrame, ltf_df: pd.DataFrame) -> Dict:
        if htf_df.empty or ltf_df.empty:
            return None
            
        last_htf = htf_df.iloc[-1]
        last_ltf = ltf_df.iloc[-1]
        prev_ltf = ltf_df.iloc[-2] if len(ltf_df) > 1 else last_ltf
        
        # ØªØ­Ù„ÛŒÙ„ Ø±ÙˆÙ†Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        htf_trend = self._analyze_enhanced_trend(last_htf)
        ltf_trend = self._analyze_enhanced_trend(last_ltf)
        
        # ØªØ­Ù„ÛŒÙ„ Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡
        momentum = self._analyze_enhanced_momentum(last_ltf)
        
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        key_levels = self._analyze_enhanced_key_levels(htf_df, ltf_df, last_ltf['close'])
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„ Ø§Ø³ØªÛŒÚ©
        candle_analysis = self._analyze_candle_patterns(ltf_df)
        
        # ØªØ­Ù„ÛŒÙ„ Ù‚Ø¯Ø±Øª Ø±ÙˆÙ†Ø¯
        trend_strength = self._analyze_trend_strength(htf_df, ltf_df)
        
        # Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ
        combined_signals = self._generate_combined_signals(htf_trend, ltf_trend, momentum, key_levels)
        
        return {
            'symbol': symbol,
            'htf_trend': htf_trend,
            'ltf_trend': ltf_trend,
            'momentum': momentum,
            'key_levels': key_levels,
            'candle_patterns': candle_analysis,
            'trend_strength': trend_strength,
            'combined_signals': combined_signals,
            'volatility': last_ltf.get('ATRr_14', 0),
            'timestamp': datetime.now(UTC).isoformat()
        }

    def _analyze_enhanced_trend(self, data: pd.Series) -> Dict:
        ema_8 = data.get('EMA_8', 0)
        ema_21 = data.get('EMA_21', 0)
        ema_50 = data.get('EMA_50', 0)
        ema_200 = data.get('EMA_200', 0)
        adx = data.get('ADX_14', 0)
        psar = data.get('PSARl_0.02_0.2', 0)
        
        # ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±ÙˆÙ†Ø¯
        ema_alignment = "ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ" if ema_8 > ema_21 > ema_50 > ema_200 else \
                       "Ù†Ø²ÙˆÙ„ÛŒ Ù‚ÙˆÛŒ" if ema_8 < ema_21 < ema_50 < ema_200 else \
                       "ØµØ¹ÙˆØ¯ÛŒ Ø¶Ø¹ÛŒÙ" if ema_8 > ema_21 else \
                       "Ù†Ø²ÙˆÙ„ÛŒ Ø¶Ø¹ÛŒÙ" if ema_8 < ema_21 else "Ø®Ù†Ø«ÛŒ"
        
        trend_strength = "Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ" if adx > 40 else "Ù‚ÙˆÛŒ" if adx > 25 else "Ù…ØªÙˆØ³Ø·" if adx > 20 else "Ø¶Ø¹ÛŒÙ"
        
        psar_signal = "ØµØ¹ÙˆØ¯ÛŒ" if psar < data.get('close', 0) else "Ù†Ø²ÙˆÙ„ÛŒ"
        
        return {
            'direction': ema_alignment,
            'strength': trend_strength,
            'adx': adx,
            'psar_signal': psar_signal,
            'ema_alignment': f"EMA8: {ema_8:.5f}, EMA21: {ema_21:.5f}, EMA50: {ema_50:.5f}"
        }

    def _analyze_enhanced_momentum(self, data: pd.Series) -> Dict:
        rsi = data.get('RSI_14', 50)
        macd_hist = data.get('MACDh_12_26_9', 0)
        stoch_k = data.get('STOCHk_14_3_3', 50)
        cci = data.get('CCI_20', 0)
        williams = data.get('WILLR_14', -50)
        
        rsi_signal = "Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯" if rsi > 70 else "Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´" if rsi < 30 else "Ø®Ù†Ø«ÛŒ"
        macd_signal = "ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ" if macd_hist > 0 and macd_hist > data.get('MACDh_12_26_9', 0) else \
                     "ØµØ¹ÙˆØ¯ÛŒ Ø¶Ø¹ÛŒÙ" if macd_hist > 0 else \
                     "Ù†Ø²ÙˆÙ„ÛŒ Ù‚ÙˆÛŒ" if macd_hist < 0 and macd_hist < data.get('MACDh_12_26_9', 0) else "Ù†Ø²ÙˆÙ„ÛŒ Ø¶Ø¹ÛŒÙ"
        stoch_signal = "Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯" if stoch_k > 80 else "Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´" if stoch_k < 20 else "Ø®Ù†Ø«ÛŒ"
        cci_signal = "ØµØ¹ÙˆØ¯ÛŒ" if cci > 100 else "Ù†Ø²ÙˆÙ„ÛŒ" if cci < -100 else "Ø®Ù†Ø«ÛŒ"
        williams_signal = "Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯" if williams > -20 else "Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´" if williams < -80 else "Ø®Ù†Ø«ÛŒ"
        
        return {
            'rsi': {'value': rsi, 'signal': rsi_signal},
            'macd': {'signal': macd_signal, 'histogram': macd_hist},
            'stochastic': {'value': stoch_k, 'signal': stoch_signal},
            'cci': {'value': cci, 'signal': cci_signal},
            'williams': {'value': williams, 'signal': williams_signal},
            'overall_momentum': self._calculate_overall_momentum(rsi, macd_hist, stoch_k, cci)
        }

    def _calculate_overall_momentum(self, rsi: float, macd_hist: float, stoch_k: float, cci: float) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ú©Ù„ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú†Ù†Ø¯ÛŒÙ† Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±"""
        score = 0
        if rsi > 50: score += 1
        if macd_hist > 0: score += 1
        if stoch_k > 50: score += 1
        if cci > 0: score += 1
        
        if score >= 3: return "ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ"
        if score == 2: return "ØµØ¹ÙˆØ¯ÛŒ Ø¶Ø¹ÛŒÙ"
        if score <= 1: return "Ù†Ø²ÙˆÙ„ÛŒ Ø¶Ø¹ÛŒÙ"
        return "Ù†Ø²ÙˆÙ„ÛŒ Ù‚ÙˆÛŒ"

    def _analyze_enhanced_key_levels(self, htf_df: pd.DataFrame, ltf_df: pd.DataFrame, current_price: float) -> Dict:
        bb_upper = ltf_df.get('BBU_20_2.0', pd.Series([0])).iloc[-1]
        bb_lower = ltf_df.get('BBL_20_2.0', pd.Series([0])).iloc[-1]
        bb_middle = ltf_df.get('BBM_20_2.0', pd.Series([0])).iloc[-1]
        
        kc_upper = ltf_df.get('KCUe_20_2', pd.Series([0])).iloc[-1]
        kc_lower = ltf_df.get('KCLe_20_2', pd.Series([0])).iloc[-1]
        
        support_1 = ltf_df.get('sup_1', pd.Series([0])).iloc[-1]
        resistance_1 = ltf_df.get('res_1', pd.Series([0])).iloc[-1]
        support_2 = ltf_df.get('sup_2', pd.Series([0])).iloc[-1]
        resistance_2 = ltf_df.get('res_2', pd.Series([0])).iloc[-1]
        
        pivot = ltf_df.get('pivot', pd.Series([0])).iloc[-1]
        r1 = ltf_df.get('r1', pd.Series([0])).iloc[-1]
        s1 = ltf_df.get('s1', pd.Series([0])).iloc[-1]
        
        return {
            'dynamic': {
                'bb_upper': bb_upper,
                'bb_lower': bb_lower,
                'bb_middle': bb_middle,
                'kc_upper': kc_upper,
                'kc_lower': kc_lower
            },
            'static': {
                'support_1': support_1,
                'resistance_1': resistance_1,
                'support_2': support_2,
                'resistance_2': resistance_2
            },
            'pivot_points': {
                'pivot': pivot,
                'r1': r1,
                's1': s1
            },
            'current_price_position': self._get_enhanced_price_position(current_price, support_1, resistance_1, bb_lower, bb_upper)
        }

    def _get_enhanced_price_position(self, price: float, support: float, resistance: float, bb_lower: float, bb_upper: float) -> str:
        if resistance == support or resistance <= support:
            return "Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø®Ù†Ø«ÛŒ"
        
        range_size = resistance - support
        position = (price - support) / range_size
        
        # ØªØ­Ù„ÛŒÙ„ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¨Ø§Ù†Ø¯Ù‡Ø§ÛŒ Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø±
        bb_position = ""
        if price < bb_lower:
            bb_position = " (Ø²ÛŒØ± Ø¨Ø§Ù†Ø¯ Ù¾Ø§ÛŒÛŒÙ†ÛŒ)"
        elif price > bb_upper:
            bb_position = " (Ø¨Ø§Ù„Ø§ÛŒ Ø¨Ø§Ù†Ø¯ Ø¨Ø§Ù„Ø§ÛŒÛŒ)"
        
        if position < 0.2:
            return "Ù†Ø²Ø¯ÛŒÚ© Ø­Ù…Ø§ÛŒØª Ø§ØµÙ„ÛŒ" + bb_position
        elif position > 0.8:
            return "Ù†Ø²Ø¯ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª Ø§ØµÙ„ÛŒ" + bb_position
        elif position < 0.4:
            return "Ù†Ø²Ø¯ÛŒÚ© Ø­Ù…Ø§ÛŒØª" + bb_position
        elif position > 0.6:
            return "Ù†Ø²Ø¯ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª" + bb_position
        else:
            return "Ø¯Ø± Ù…ÛŒØ§Ù†Ù‡ Ø±Ù†Ø¬" + bb_position

    def _analyze_trend_strength(self, htf_df: pd.DataFrame, ltf_df: pd.DataFrame) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ù‚Ø¯Ø±Øª Ø±ÙˆÙ†Ø¯"""
        htf_adx = htf_df.get('ADX_14', pd.Series([0])).iloc[-1]
        ltf_adx = ltf_df.get('ADX_14', pd.Series([0])).iloc[-1]
        
        htf_trend_dir = "ØµØ¹ÙˆØ¯ÛŒ" if htf_df.get('EMA_21', pd.Series([0])).iloc[-1] > htf_df.get('EMA_50', pd.Series([0])).iloc[-1] else "Ù†Ø²ÙˆÙ„ÛŒ"
        ltf_trend_dir = "ØµØ¹ÙˆØ¯ÛŒ" if ltf_df.get('EMA_21', pd.Series([0])).iloc[-1] > ltf_df.get('EMA_50', pd.Series([0])).iloc[-1] else "Ù†Ø²ÙˆÙ„ÛŒ"
        
        trend_alignment = "Ù‡Ù…Ø³Ùˆ" if htf_trend_dir == ltf_trend_dir else "ØºÛŒØ±Ù‡Ù…Ø³Ùˆ"
        
        return {
            'htf_strength': "Ù‚ÙˆÛŒ" if htf_adx > 25 else "Ø¶Ø¹ÛŒÙ",
            'ltf_strength': "Ù‚ÙˆÛŒ" if ltf_adx > 25 else "Ø¶Ø¹ÛŒÙ",
            'trend_alignment': trend_alignment,
            'overall_strength': "Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ" if htf_adx > 25 and ltf_adx > 25 and trend_alignment == "Ù‡Ù…Ø³Ùˆ" else "Ù‚ÙˆÛŒ" if (htf_adx > 25 or ltf_adx > 25) else "Ø¶Ø¹ÛŒÙ"
        }

    def _generate_combined_signals(self, htf_trend: Dict, ltf_trend: Dict, momentum: Dict, key_levels: Dict) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ"""
        signals = []
        
        # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø±ÙˆÙ†Ø¯
        if htf_trend['direction'] == ltf_trend['direction'] and "ØµØ¹ÙˆØ¯ÛŒ" in htf_trend['direction']:
            signals.append("Ù‡Ù…Ø³ÙˆÛŒÛŒ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ")
        elif htf_trend['direction'] == ltf_trend['direction'] and "Ù†Ø²ÙˆÙ„ÛŒ" in htf_trend['direction']:
            signals.append("Ù‡Ù…Ø³ÙˆÛŒÛŒ Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ")
        
        # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…ÙˆÙ…Ù†ØªÙˆÙ…
        if momentum['overall_momentum'] == "ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ":
            signals.append("Ù…ÙˆÙ…Ù†ØªÙˆÙ… ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ")
        elif momentum['overall_momentum'] == "Ù†Ø²ÙˆÙ„ÛŒ Ù‚ÙˆÛŒ":
            signals.append("Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ù†Ø²ÙˆÙ„ÛŒ Ù‚ÙˆÛŒ")
        
        # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚ÛŒÙ…Øª
        price_pos = key_levels['current_price_position']
        if "Ù†Ø²Ø¯ÛŒÚ© Ø­Ù…Ø§ÛŒØª" in price_pos and "ØµØ¹ÙˆØ¯ÛŒ" in momentum['overall_momentum']:
            signals.append("Ù…ÙˆÙ‚Ø¹ÛŒØª Ø®Ø±ÛŒØ¯ Ø¯Ø± Ø­Ù…Ø§ÛŒØª")
        elif "Ù†Ø²Ø¯ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª" in price_pos and "Ù†Ø²ÙˆÙ„ÛŒ" in momentum['overall_momentum']:
            signals.append("Ù…ÙˆÙ‚Ø¹ÛŒØª ÙØ±ÙˆØ´ Ø¯Ø± Ù…Ù‚Ø§ÙˆÙ…Øª")
        
        return signals

    def _analyze_candle_patterns(self, df: pd.DataFrame) -> Dict:
        if len(df) < 3:
            return {'patterns': [], 'current_candle': {}, 'recent_patterns': []}
            
        last_candle = df.iloc[-1]
        patterns = []
        
        candle_indicators = [col for col in df.columns if col.startswith('CDL_')]
        for indicator in candle_indicators:
            if abs(last_candle.get(indicator, 0)) > 0:
                pattern_name = indicator.replace('CDL_', '')
                direction = "ØµØ¹ÙˆØ¯ÛŒ" if last_candle[indicator] > 0 else "Ù†Ø²ÙˆÙ„ÛŒ"
                patterns.append(f"{pattern_name} ({direction})")
        
        current_candle = self._analyze_single_candle(df.iloc[-1])
        
        return {
            'patterns': patterns,
            'current_candle': current_candle,
            'recent_patterns': patterns[-3:] if patterns else [],
            'pattern_strength': "Ù‚ÙˆÛŒ" if len(patterns) > 0 and any("ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ" in p or "Ù†Ø²ÙˆÙ„ÛŒ Ù‚ÙˆÛŒ" in p for p in patterns) else "Ù…ØªÙˆØ³Ø·" if patterns else "Ø¶Ø¹ÛŒÙ"
        }

    def _analyze_single_candle(self, candle: pd.Series) -> Dict:
        open_price = candle.get('open', 0)
        close = candle.get('close', 0)
        high = candle.get('high', 0)
        low = candle.get('low', 0)
        
        body_size = abs(close - open_price)
        total_range = high - low
        
        if total_range == 0:
            return {"type": "ØªØ¹Ø±ÛŒÙ Ù†Ø´Ø¯Ù‡", "direction": "Ø®Ù†Ø«ÛŒ", "body_ratio": 0, "strength": "Ø¶Ø¹ÛŒÙ"}
            
        body_ratio = body_size / total_range
        
        upper_shadow = high - max(open_price, close)
        lower_shadow = min(open_price, close) - low
        
        if body_ratio < 0.1 and upper_shadow > 0 and lower_shadow > 0:
            candle_type = "Ø¯ÙˆØ¬ÛŒ"
        elif body_ratio < 0.3 and lower_shadow > 2 * body_size:
            candle_type = "Ú†Ú©Ø´"
        elif body_ratio < 0.3 and upper_shadow > 2 * body_size:
            candle_type = "Ø³ØªØ§Ø±Ù‡ Ø«Ø§Ù‚Ø¨"
        elif body_ratio > 0.7:
            candle_type = "Ù…Ø§Ø±ÙˆØ¨ÙˆØ²Ùˆ"
        else:
            candle_type = "Ø¹Ø§Ø¯ÛŒ"
            
        direction = "ØµØ¹ÙˆØ¯ÛŒ" if close > open_price else "Ù†Ø²ÙˆÙ„ÛŒ"
        
        shadow_ratio = (upper_shadow + lower_shadow) / total_range if total_range > 0 else 0
        
        return {
            'type': candle_type,
            'direction': direction,
            'body_ratio': round(body_ratio, 2),
            'shadow_ratio': round(shadow_ratio, 2),
            'strength': "Ù‚ÙˆÛŒ" if body_ratio > 0.6 else "Ù…ØªÙˆØ³Ø·" if body_ratio > 0.3 else "Ø¶Ø¹ÛŒÙ"
        }

# =================================================================================
# --- Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª AI ØªØ±Ú©ÛŒØ¨ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ---
# =================================================================================

class AdvancedHybridAIManager:
    def __init__(self, gemini_api_key: str, cloudflare_api_key: str):
        self.gemini_api_key = gemini_api_key
        self.cloudflare_api_key = cloudflare_api_key
        self.gemini_model = GEMINI_MODEL
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Cloudflare
        self.cloudflare_account_id = os.getenv("CLOUDFLARE_ACCOUNT_ID", "your_account_id")
        self.cloudflare_model_name = IMPROVED_CLOUDFLARE_MODELS[0]
        self.fallback_models = IMPROVED_CLOUDFLARE_MODELS[1:]
        self.current_model_index = 0
        
        genai.configure(api_key=gemini_api_key)
        
        # Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯
        self.analysis_cache = {}
        self.cache_timeout = 300  # 5 minutes
    
    async def get_enhanced_analysis(self, symbol: str, technical_analysis: Dict) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØ±Ú©ÛŒØ¨ÛŒ"""
        cache_key = f"{symbol}_{hash(str(technical_analysis))}"
        current_time = time.time()
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´
        if cache_key in self.analysis_cache:
            cached_data, timestamp = self.analysis_cache[cache_key]
            if current_time - timestamp < self.cache_timeout:
                return cached_data
        
        tasks = [
            self._get_enhanced_gemini_analysis(symbol, technical_analysis),
            self._get_enhanced_cloudflare_analysis(symbol, technical_analysis)
        ]
        
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            gemini_result, cloudflare_result = results
            
            if isinstance(gemini_result, Exception):
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Gemini Ø¨Ø±Ø§ÛŒ {symbol}: {gemini_result}")
                gemini_result = None
            if isinstance(cloudflare_result, Exception):
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Cloudflare Ø¨Ø±Ø§ÛŒ {symbol}: {cloudflare_result}")
                cloudflare_result = None
            
            combined_result = self._combine_enhanced_analyses(symbol, gemini_result, cloudflare_result, technical_analysis)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
            if combined_result:
                self.analysis_cache[cache_key] = (combined_result, current_time)
            
            return combined_result
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None
    
    async def _get_enhanced_gemini_analysis(self, symbol: str, technical_analysis: Dict) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Gemini"""
        try:
            prompt = self._create_enhanced_prompt(symbol, technical_analysis, "Gemini")
            model = genai.GenerativeModel(self.gemini_model)
            
            response = await asyncio.to_thread(
                model.generate_content,
                prompt,
                request_options={'timeout': 60}
            )
            
            return self._parse_enhanced_ai_response(response.text, symbol, "Gemini")
            
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Gemini Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None
    
    async def _get_enhanced_cloudflare_analysis(self, symbol: str, technical_analysis: Dict, retry_count: int = 0) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Cloudflare AI Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª retry"""
        if not self.cloudflare_api_key or self.cloudflare_account_id == "your_account_id":
            logging.warning("Ú©Ù„ÛŒØ¯ ÛŒØ§ Ø´Ù†Ø§Ø³Ù‡ Ø­Ø³Ø§Ø¨ Cloudflare API ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª")
            return None
            
        try:
            prompt = self._create_enhanced_prompt(symbol, technical_analysis, "Cloudflare")
            
            headers = {
                "Authorization": f"Bearer {self.cloudflare_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "messages": [
                    {
                        "role": "system", 
                        "content": "You are an expert forex trading analyst with 20 years experience. Provide precise analysis in valid JSON format only."
                    },
                    {"role": "user", "content": prompt}
                ],
                "stream": False,
                "max_tokens": 1000
            }
            
            current_model = self.fallback_models[self.current_model_index] if retry_count > 0 else self.cloudflare_model_name
            cloudflare_url = f"https://api.cloudflare.com/client/v4/accounts/{self.cloudflare_account_id}/ai/run/{current_model}"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(cloudflare_url, headers=headers, json=payload, timeout=60) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = self._extract_cloudflare_response(data)
                        if content:
                            return self._parse_enhanced_ai_response(content, symbol, f"Cloudflare ({current_model})")
                        else:
                            raise Exception("Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ Ø§Ø² Cloudflare")
                    else:
                        error_text = await response.text()
                        raise Exception(f"HTTP {response.status}: {error_text}")
                        
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Cloudflare Ø¨Ø±Ø§ÛŒ {symbol} (ØªÙ„Ø§Ø´ {retry_count + 1}): {e}")
            
            # ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ Ù…Ø¯Ù„ fallback
            if retry_count < len(self.fallback_models) - 1:
                self.current_model_index = (self.current_model_index + 1) % len(self.fallback_models)
                await asyncio.sleep(1)
                return await self._get_enhanced_cloudflare_analysis(symbol, technical_analysis, retry_count + 1)
            else:
                return None

    def _extract_cloudflare_response(self, data: Dict) -> Optional[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§Ø³Ø® Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Cloudflare"""
        if "result" in data and "response" in data["result"]:
            return data["result"]["response"]
        elif "response" in data:
            return data["response"]
        else:
            logging.warning(f"ÙØ±Ù…Øª Ù¾Ø§Ø³Ø® Cloudflare Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª: {data}")
            return None

    def _create_enhanced_prompt(self, symbol: str, technical_analysis: Dict, ai_name: str) -> str:
        base_currency, quote_currency = symbol.split('/')
        
        return f"""
Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø§Ø²Ø§Ø± ÙØ§Ø±Ú©Ø³ Ø¨Ø§ Û²Û° Ø³Ø§Ù„ ØªØ¬Ø±Ø¨Ù‡ØŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬ÙØª Ø§Ø±Ø² {symbol} Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· ÛŒÚ© Ø¢Ø¨Ø¬Ú©Øª JSON Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯.

ğŸ“Š **ÙˆØ¶Ø¹ÛŒØª ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ {symbol}:**

ğŸ¯ **Ø±ÙˆÙ†Ø¯Ù‡Ø§:**
- Ø±ÙˆÙ†Ø¯ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª (HTF): {technical_analysis['htf_trend']['direction']} - Ù‚Ø¯Ø±Øª: {technical_analysis['htf_trend']['strength']}
- Ø±ÙˆÙ†Ø¯ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª (LTF): {technical_analysis['ltf_trend']['direction']} - Ù‚Ø¯Ø±Øª: {technical_analysis['ltf_trend']['strength']}
- Ù‡Ù…Ø³ÙˆÛŒÛŒ Ø±ÙˆÙ†Ø¯Ù‡Ø§: {technical_analysis['trend_strength']['trend_alignment']}
- Ù‚Ø¯Ø±Øª Ú©Ù„ÛŒ: {technical_analysis['trend_strength']['overall_strength']}

âš¡ **Ù…ÙˆÙ…Ù†ØªÙˆÙ…:**
- RSI: {technical_analysis['momentum']['rsi']['value']:.1f} ({technical_analysis['momentum']['rsi']['signal']})
- MACD: {technical_analysis['momentum']['macd']['signal']}
- Stochastic: {technical_analysis['momentum']['stochastic']['value']:.1f} ({technical_analysis['momentum']['stochastic']['signal']})
- Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ú©Ù„ÛŒ: {technical_analysis['momentum']['overall_momentum']}

ğŸ“ˆ **Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ:**
- Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚ÛŒÙ…Øª: {technical_analysis['key_levels']['current_price_position']}
- Ù…Ù‚Ø§ÙˆÙ…Øª Û±: {technical_analysis['key_levels']['static']['resistance_1']:.5f}
- Ø­Ù…Ø§ÛŒØª Û±: {technical_analysis['key_levels']['static']['support_1']:.5f}
- Ù…Ù‚Ø§ÙˆÙ…Øª Û²: {technical_analysis['key_levels']['static']['resistance_2']:.5f}
- Ø­Ù…Ø§ÛŒØª Û²: {technical_analysis['key_levels']['static']['support_2']:.5f}
- Ù¾ÛŒÙˆØª: {technical_analysis['key_levels']['pivot_points']['pivot']:.5f}

ğŸ•¯ï¸ **Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„â€ŒØ§Ø³ØªÛŒÚ©:**
- Ú©Ù†Ø¯Ù„ ÙØ¹Ù„ÛŒ: {technical_analysis['candle_patterns']['current_candle']['type']} ({technical_analysis['candle_patterns']['current_candle']['direction']})
- Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {', '.join(technical_analysis['candle_patterns']['patterns'][-2:])}

ğŸ’¡ **Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ:**
{chr(10).join(['- ' + signal for signal in technical_analysis['combined_signals']])}

**Ù„Ø·ÙØ§Ù‹ Ù¾Ø§Ø³Ø® Ø±Ø§ ÙÙ‚Ø· Ø¯Ø± Ù‚Ø§Ù„Ø¨ JSON Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ (Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† Ù…ØªÙ† Ø§Ø¶Ø§ÙÛŒ):**

{{
  "SYMBOL": "{symbol}",
  "ACTION": "BUY/SELL/HOLD",
  "CONFIDENCE": 1-10,
  "ENTRY_ZONE": "Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¹Ø¯Ø¯ÛŒ (Ù…Ø«Ø§Ù„: 1.12340-1.12360)",
  "STOP_LOSS": "Ø¹Ø¯Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ Ø¯Ù‚ÛŒÙ‚ (Ù…Ø«Ø§Ù„: 1.12050)", 
  "TAKE_PROFIT_1": "Ø¹Ø¯Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ Ø¯Ù‚ÛŒÙ‚ (Ù…Ø«Ø§Ù„: 1.12800)",
  "TAKE_PROFIT_2": "Ø¹Ø¯Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ Ø¯Ù‚ÛŒÙ‚ (Ù…Ø«Ø§Ù„: 1.13000)",
  "RISK_REWARD_RATIO": "Ù†Ø³Ø¨Øª Ø¹Ø¯Ø¯ÛŒ (Ù…Ø«Ø§Ù„: 1.8)",
  "ANALYSIS": "ØªØ­Ù„ÛŒÙ„ Ù…Ø®ØªØµØ± Ùˆ ØªØ®ØµØµÛŒ ÙØ§Ø±Ø³ÛŒ",
  "EXPIRATION_H": "Ø¹Ø¯Ø¯ ØµØ­ÛŒØ­ (Ù…Ø«Ø§Ù„: 6)",
  "PRIORITY": "HIGH/MEDIUM/LOW"
}}
"""

    def _parse_enhanced_ai_response(self, response: str, symbol: str, ai_name: str) -> Optional[Dict]:
        try:
            cleaned_response = response.strip()
            
            json_patterns = [
                r'```json\s*(\{.*?\})\s*```',
                r'```\s*(\{.*?\})\s*```',
                r'(\{.*\})'
            ]
            
            json_match = None
            for pattern in json_patterns:
                json_match = re.search(pattern, cleaned_response, re.DOTALL)
                if json_match:
                    break
            
            if json_match:
                json_str = json_match.group(1)
                signal_data = json.loads(json_str)
                
                if not self._validate_enhanced_signal_data(signal_data, symbol):
                    return None
                
                signal_data['ai_model'] = ai_name
                signal_data['timestamp'] = datetime.now(UTC).isoformat()
                logging.info(f"âœ… {ai_name} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø±Ø§ÛŒ {symbol}: {signal_data.get('ACTION', 'HOLD')}")
                return signal_data
            else:
                logging.warning(f"âŒ Ù¾Ø§Ø³Ø® {ai_name} Ø¨Ø±Ø§ÛŒ {symbol} ÙØ§Ù‚Ø¯ ÙØ±Ù…Øª JSON Ø¨ÙˆØ¯")
                return None
                
        except json.JSONDecodeError as e:
            logging.error(f"Ø®Ø·Ø§ÛŒ JSON Ø¯Ø± Ù¾Ø§Ø³Ø® {ai_name} Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø® {ai_name} Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            return None

    def _validate_enhanced_signal_data(self, signal_data: Dict, symbol: str) -> bool:
        required_fields = ['SYMBOL', 'ACTION', 'CONFIDENCE']
        
        for field in required_fields:
            if field not in signal_data:
                logging.warning(f"ÙÛŒÙ„Ø¯ Ø¶Ø±ÙˆØ±ÛŒ {field} Ø¯Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„ {symbol} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return False
        
        action = signal_data['ACTION'].upper()
        if action not in ['BUY', 'SELL', 'HOLD']:
            logging.warning(f"ACTION Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ {symbol}: {action}")
            return False
        
        try:
            confidence = float(signal_data['CONFIDENCE'])
            if not (1 <= confidence <= 10):
                logging.warning(f"CONFIDENCE Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¨Ø±Ø§ÛŒ {symbol}: {confidence}")
                return False
        except (ValueError, TypeError):
            logging.warning(f"CONFIDENCE Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ {symbol}: {signal_data['CONFIDENCE']}")
            return False
        
        return True

    def _combine_enhanced_analyses(self, symbol: str, gemini_result: Dict, cloudflare_result: Dict, technical_analysis: Dict) -> Optional[Dict]:
        valid_results = []
        
        if gemini_result and self._validate_enhanced_signal_data(gemini_result, symbol):
            valid_results.append(('Gemini', gemini_result))
        
        if cloudflare_result and self._validate_enhanced_signal_data(cloudflare_result, symbol):
            valid_results.append(('Cloudflare', cloudflare_result))
        
        if not valid_results:
            logging.info(f"Ù‡ÛŒÚ† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI Ø¨Ø±Ø§ÛŒ {symbol} Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return self._create_hold_signal(symbol, technical_analysis)
        
        if len(valid_results) == 1:
            model_name, result = valid_results[0]
            return self._enhance_single_model_result(result, model_name, technical_analysis)
        
        gemini_data = valid_results[0][1] if valid_results[0][0] == 'Gemini' else valid_results[1][1]
        cloudflare_data = valid_results[0][1] if valid_results[0][0] == 'Cloudflare' else valid_results[1][1]
        
        gemini_action = gemini_data.get('ACTION', 'HOLD').upper()
        cloudflare_action = cloudflare_data.get('ACTION', 'HOLD').upper()
        
        if gemini_action == cloudflare_action:
            return self._create_consensus_signal(symbol, gemini_data, cloudflare_data, technical_analysis)
        else:
            return self._resolve_enhanced_conflict(symbol, gemini_data, cloudflare_data, technical_analysis)

    def _create_hold_signal(self, symbol: str, technical_analysis: Dict) -> Dict:
        return {
            'SYMBOL': symbol,
            'ACTION': 'HOLD',
            'CONFIDENCE': 0,
            'CONSENSUS': False,
            'ANALYSIS': 'Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹ØªØ¨Ø± Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI',
            'TIMESTAMP': datetime.now(UTC).isoformat(),
            'TECHNICAL_CONTEXT': technical_analysis.get('combined_signals', [])
        }

    def _enhance_single_model_result(self, result: Dict, model_name: str, technical_analysis: Dict) -> Dict:
        result['CONSENSUS'] = False
        result['MODEL_SOURCE'] = f"Single: {model_name}"
        
        # Ú©Ø§Ù‡Ø´ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÚ© Ù…Ø¯Ù„ÛŒ
        original_confidence = float(result.get('CONFIDENCE', 5))
        result['CONFIDENCE'] = max(1, original_confidence - 2)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù†ÛŒÚ©Ø§Ù„
        if 'TECHNICAL_CONTEXT' not in result:
            result['TECHNICAL_CONTEXT'] = technical_analysis.get('combined_signals', [])
        
        return result

    def _create_consensus_signal(self, symbol: str, gemini_data: Dict, cloudflare_data: Dict, technical_analysis: Dict) -> Dict:
        averaged_signal = self._average_enhanced_signals(symbol, gemini_data, cloudflare_data)
        averaged_signal['CONSENSUS'] = True
        averaged_signal['MODELS_AGREE'] = True
        averaged_signal['MODEL_SOURCE'] = "Gemini + Cloudflare Consensus"
        averaged_signal['PRIORITY'] = self._calculate_priority(averaged_signal, technical_analysis)
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚
        original_confidence = float(averaged_signal.get('CONFIDENCE', 5))
        averaged_signal['CONFIDENCE'] = min(10, original_confidence + 1)
        
        averaged_signal['FINAL_ANALYSIS'] = f"ØªÙˆØ§ÙÙ‚ Ú©Ø§Ù…Ù„ Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ - Ø³ÛŒÚ¯Ù†Ø§Ù„ {gemini_data['ACTION']} Ø¨Ø§ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø§Ù„Ø§"
        averaged_signal['TECHNICAL_CONTEXT'] = technical_analysis.get('combined_signals', [])
        
        return averaged_signal

    def _resolve_enhanced_conflict(self, symbol: str, gemini_data: Dict, cloudflare_data: Dict, technical_analysis: Dict) -> Dict:
        gemini_conf = float(gemini_data.get('CONFIDENCE', 5))
        cloudflare_conf = float(cloudflare_data.get('CONFIDENCE', 5))
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø¹ØªÙ…Ø§Ø¯ Ùˆ Ù‡Ù…Ø³ÙˆÛŒÛŒ Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
        tech_signals = technical_analysis.get('combined_signals', [])
        gemini_score = self._calculate_model_score(gemini_data, gemini_conf, tech_signals)
        cloudflare_score = self._calculate_model_score(cloudflare_data, cloudflare_conf, tech_signals)
        
        if gemini_score >= cloudflare_score:
            selected = gemini_data
            selected_model = 'Gemini'
        else:
            selected = cloudflare_data
            selected_model = 'Cloudflare'
        
        selected['CONSENSUS'] = False
        selected['MODELS_AGREE'] = False
        selected['MODEL_SOURCE'] = f"Conflict Resolution: {selected_model}"
        selected['CONFIDENCE'] = max(1, float(selected.get('CONFIDENCE', 5)) - 1)
        selected['PRIORITY'] = 'LOW'
        selected['ANALYSIS'] = f"Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø§Ø² {selected_model} - ØªØ¶Ø§Ø¯ Ø¨Ø§ Ù…Ø¯Ù„ Ø¯ÛŒÚ¯Ø± - Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ£ÛŒÛŒØ¯"
        selected['TECHNICAL_CONTEXT'] = tech_signals
        
        return selected

    def _calculate_model_score(self, signal_data: Dict, confidence: float, tech_signals: List[str]) -> float:
        score = confidence
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ù…Ø³ÙˆÛŒÛŒ Ø¨Ø§ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
        action = signal_data.get('ACTION', '').upper()
        if action == 'BUY' and any('ØµØ¹ÙˆØ¯ÛŒ' in signal or 'Ø®Ø±ÛŒØ¯' in signal for signal in tech_signals):
            score += 2
        elif action == 'SELL' and any('Ù†Ø²ÙˆÙ„ÛŒ' in signal or 'ÙØ±ÙˆØ´' in signal for signal in tech_signals):
            score += 2
        
        return score

    def _calculate_priority(self, signal: Dict, technical_analysis: Dict) -> str:
        confidence = float(signal.get('CONFIDENCE', 5))
        trend_strength = technical_analysis.get('trend_strength', {}).get('overall_strength', 'Ø¶Ø¹ÛŒÙ')
        
        if confidence >= 8 and trend_strength in ['Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ', 'Ù‚ÙˆÛŒ']:
            return 'HIGH'
        elif confidence >= 6:
            return 'MEDIUM'
        else:
            return 'LOW'

    def _average_enhanced_signals(self, symbol: str, gemini_data: Dict, cloudflare_data: Dict) -> Dict:
        averaged = {'SYMBOL': symbol}
        
        averaged['ACTION'] = gemini_data['ACTION']
        
        gemini_conf = float(gemini_data.get('CONFIDENCE', 5))
        cloudflare_conf = float(cloudflare_data.get('CONFIDENCE', 5))
        averaged['CONFIDENCE'] = round((gemini_conf + cloudflare_conf) / 2, 1)
        
        numeric_fields = ['ENTRY_ZONE', 'STOP_LOSS', 'TAKE_PROFIT_1', 'TAKE_PROFIT_2', 'EXPIRATION_H']
        
        for field in numeric_fields:
            gemini_val = self._extract_numeric_value(gemini_data.get(field, '0'))
            cloudflare_val = self._extract_numeric_value(cloudflare_data.get(field, '0'))
            
            if gemini_val is not None and cloudflare_val is not None:
                avg_val = (gemini_val + cloudflare_val) / 2
                if field == 'EXPIRATION_H':
                    averaged[field] = int(round(avg_val))
                else:
                    averaged[field] = round(avg_val, 5)
            elif gemini_val is not None:
                averaged[field] = gemini_val
            elif cloudflare_val is not None:
                averaged[field] = cloudflare_val
        
        averaged['RISK_REWARD_RATIO'] = self._calculate_risk_reward(
            averaged.get('STOP_LOSS', 0), 
            averaged.get('TAKE_PROFIT_1', 0),
            gemini_data.get('close', 0)
        )
        
        averaged['GEMINI_ANALYSIS'] = gemini_data.get('ANALYSIS', '')
        averaged['CLOUDFLARE_ANALYSIS'] = cloudflare_data.get('ANALYSIS', '')
        averaged['ANALYSIS'] = f"ØªØ±Ú©ÛŒØ¨ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§: {gemini_data.get('ANALYSIS', '')}"
        
        return averaged

    def _extract_numeric_value(self, value: str) -> Optional[float]:
        if isinstance(value, (int, float)):
            return float(value)
        if not isinstance(value, str):
            return None
            
        match = re.search(r'[-+]?\d*\.\d+|\d+', value.replace(',', ''))
        if match:
            try:
                return float(match.group(0))
            except (ValueError, TypeError):
                return None
        return None

    def _calculate_risk_reward(self, stop_loss: float, take_profit: float, entry: float) -> float:
        if not all([stop_loss, take_profit, entry]) or stop_loss == take_profit:
            return 1.0
        
        risk = abs(entry - stop_loss)
        reward = abs(take_profit - entry)
        
        if risk == 0:
            return 1.0
        
        return round(reward / risk, 2)

# =================================================================================
# --- Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚ ---
# =================================================================================

class ConsensusSignalManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚ Ùˆ Ø¨Ø¯ÙˆÙ† ØªÙˆØ§ÙÙ‚"""
    
    def __init__(self):
        self.consensus_file = "consensus_signals.json"
        self.non_consensus_file = "non_consensus_signals.json"
        self.consensus_threshold = 7  # Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚
        
    def categorize_signals(self, signals: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙˆØ§ÙÙ‚"""
        consensus_signals = []
        non_consensus_signals = []
        
        for signal in signals:
            if self._is_consensus_signal(signal):
                consensus_signals.append(signal)
            else:
                non_consensus_signals.append(signal)
        
        return consensus_signals, non_consensus_signals
    
    def _is_consensus_signal(self, signal: Dict) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¯Ø§Ø±Ø§ÛŒ ØªÙˆØ§ÙÙ‚ Ø§Ø³Øª"""
        # Ø´Ø±Ø· 1: Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ù… ØªÙˆØ§ÙÙ‚ Ø¯Ø§Ø±Ù†Ø¯
        models_agree = signal.get('MODELS_AGREE', False)
        consensus_flag = signal.get('CONSENSUS', False)
        
        # Ø´Ø±Ø· 2: Ø³Ø·Ø­ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø§Ù„Ø§
        confidence = float(signal.get('CONFIDENCE', 0))
        high_confidence = confidence >= self.consensus_threshold
        
        # Ø´Ø±Ø· 3: Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø§Ø² Ù†ÙˆØ¹ HOLD Ù†ÛŒØ³Øª
        not_hold = signal.get('ACTION', 'HOLD') != 'HOLD'
        
        # Ø´Ø±Ø· 4: Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§
        high_priority = signal.get('PRIORITY', 'LOW') in ['HIGH', 'MEDIUM']
        
        return (models_agree or consensus_flag) and high_confidence and not_hold and high_priority
    
    def save_categorized_signals(self, consensus_signals: List[Dict], non_consensus_signals: List[Dict]):
        """Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡"""
        # Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚
        if consensus_signals:
            self._save_to_file(consensus_signals, self.consensus_file)
            logging.info(f"âœ… {len(consensus_signals)} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø§ ØªÙˆØ§ÙÙ‚ Ø¯Ø± {self.consensus_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ§ÙÙ‚
        if non_consensus_signals:
            self._save_to_file(non_consensus_signals, self.non_consensus_file)
            logging.info(f"ğŸ“Š {len(non_consensus_signals)} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø¯ÙˆÙ† ØªÙˆØ§ÙÙ‚ Ø¯Ø± {self.non_consensus_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            
    def _save_to_file(self, signals: List[Dict], filename: str):
        """Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„"""
        try:
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø¹ØªÙ…Ø§Ø¯
            sorted_signals = sorted(signals, key=lambda x: float(x.get('CONFIDENCE', 0)), reverse=True)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(sorted_signals, f, indent=4, ensure_ascii=False, default=str)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ {filename}: {e}")

# =================================================================================
# --- Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ ØªØ­Ù„ÛŒÙ„Ú¯Ø± ÙØ§Ø±Ú©Ø³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ---
# =================================================================================

class AdvancedForexAnalyzer:
    def __init__(self):
        self.api_rate_limiter = AsyncRateLimiter(rate_limit=6, period=60)  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ GitHub Actions
        self.cache_manager = SmartCacheManager(CACHE_FILE, CACHE_DURATION_HOURS)
        self.technical_analyzer = EnhancedTechnicalAnalyzer()
        self.ai_manager = AdvancedHybridAIManager(google_api_key, CLOUDFLARE_AI_API_KEY)
        self.signal_manager = ConsensusSignalManager()

    async def analyze_pair(self, pair: str) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ ÛŒÚ© Ø¬ÙØª Ø§Ø±Ø²"""
        if self.cache_manager.is_pair_on_cooldown(pair):
            return None
        
        logging.info(f"ğŸ” Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ {pair}")
        
        try:
            # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±
            htf_df = await self.get_market_data_async(pair, HIGH_TIMEFRAME)
            ltf_df = await self.get_market_data_async(pair, LOW_TIMEFRAME)
            
            if htf_df is None or ltf_df is None or htf_df.empty or ltf_df.empty:
                logging.warning(f"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨Ø±Ø§ÛŒ {pair} Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
                return None
            
            # ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡
            htf_df_processed = self.technical_analyzer.calculate_enhanced_indicators(htf_df)
            ltf_df_processed = self.technical_analyzer.calculate_enhanced_indicators(ltf_df)
            
            if htf_df_processed is None or ltf_df_processed is None:
                logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ {pair}")
                return None
            
            technical_analysis = self.technical_analyzer.generate_enhanced_analysis(pair, htf_df_processed, ltf_df_processed)
            
            if not technical_analysis:
                logging.warning(f"ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¨Ø±Ø§ÛŒ {pair} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
                return None
            
            # ØªØ­Ù„ÛŒÙ„ ØªØ±Ú©ÛŒØ¨ÛŒ AI
            ai_analysis = await self.ai_manager.get_enhanced_analysis(pair, technical_analysis)
            
            if ai_analysis and ai_analysis.get('ACTION') != 'HOLD':
                self.cache_manager.update_cache(pair, ai_analysis)
                logging.info(f"âœ… Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø¨Ø±Ø§ÛŒ {pair}: {ai_analysis['ACTION']} (Ø§Ø¹ØªÙ…Ø§Ø¯: {ai_analysis.get('CONFIDENCE', 0)})")
                return ai_analysis
            else:
                logging.info(f"ğŸ” Ù‡ÛŒÚ† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø¨Ø±Ø§ÛŒ {pair} Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯")
                return None
                
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ {pair}: {e}")
            return None

    async def get_market_data_async(self, symbol: str, interval: str, retries: int = 2) -> Optional[pd.DataFrame]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø¨Ù‡ ØµÙˆØ±Øª Ø¢Ø³Ù†Ú©Ø±ÙˆÙ† - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ GitHub Actions"""
        for attempt in range(retries):
            try:
                async with self.api_rate_limiter:
                    url = f'https://api.twelvedata.com/time_series?symbol={symbol}&interval={interval}&outputsize={CANDLES_TO_FETCH}&apikey={TWELVEDATA_API_KEY}'
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.get(url, timeout=30) as response:  # Ú©Ø§Ù‡Ø´ timeout
                            if response.status == 200:
                                data = await response.json()
                                if 'values' in data and data['values']:
                                    df = pd.DataFrame(data['values'])
                                    df = df.iloc[::-1].reset_index(drop=True)
                                    
                                    numeric_columns = ['open', 'high', 'low', 'close']
                                    for col in numeric_columns:
                                        if col in df.columns:
                                            df[col] = pd.to_numeric(df[col], errors='coerce')
                                    
                                    if 'datetime' in df.columns:
                                        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
                                    
                                    df = df.dropna(subset=numeric_columns)
                                    
                                    if len(df) > 50:  # Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø§Ø¯Ù‡ Ù„Ø§Ø²Ù…
                                        return df
                                    else:
                                        logging.warning(f"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} Ù†Ø§Ú©Ø§ÙÛŒ Ø§Ø³Øª")
                                        return None
                                else:
                                    logging.warning(f"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
                                    return None
                            else:
                                logging.warning(f"Ø®Ø·Ø§ÛŒ HTTP {response.status} Ø¨Ø±Ø§ÛŒ {symbol}")
                                if response.status == 429:
                                    await asyncio.sleep(15)  # Ø§ÙØ²Ø§ÛŒØ´ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ rate limit
                                
            except Exception as e:
                logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} (ØªÙ„Ø§Ø´ {attempt + 1}): {e}")
                await asyncio.sleep(3)
        
        logging.error(f"Ø¹Ø¯Ù… Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} Ù¾Ø³ Ø§Ø² {retries} ØªÙ„Ø§Ø´")
        return None

    async def analyze_all_pairs(self, pairs: List[str]) -> List[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ù‡ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ GitHub Actions"""
        logging.info(f"ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù…ÙˆØ§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ {len(pairs)} Ø¬ÙØª Ø§Ø±Ø²")
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† concurrent tasks Ø¨Ø±Ø§ÛŒ GitHub Actions
        semaphore = asyncio.Semaphore(2)  # Ú©Ø§Ù‡Ø´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overload
        
        async def bounded_analyze(pair):
            async with semaphore:
                result = await self.analyze_pair(pair)
                await asyncio.sleep(2)  # Ø§ÙØ²Ø§ÛŒØ´ ØªØ£Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø§Ø­ØªØ±Ø§Ù… Ø¨Ù‡ rate limits
                return result
        
        tasks = [bounded_analyze(pair) for pair in pairs]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ Ù…ÙˆÙÙ‚
        valid_signals = []
        for result in results:
            if isinstance(result, Dict) and result.get('ACTION') != 'HOLD':
                valid_signals.append(result)
            elif isinstance(result, Exception):
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„: {result}")
        
        logging.info(f"ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. {len(valid_signals)} Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹ØªØ¨Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
        return valid_signals

    def generate_comprehensive_report(self, signals: List[Dict]):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø§Ø² ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§"""
        consensus_signals, non_consensus_signals = self.signal_manager.categorize_signals(signals)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡
        self.signal_manager.save_categorized_signals(consensus_signals, non_consensus_signals)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ø®Ù„Ø§ØµÙ‡
        report = {
            'timestamp': datetime.now(UTC).isoformat(),
            'total_analyzed_pairs': len(CURRENCY_PAIRS_TO_ANALYZE),
            'total_signals': len(signals),
            'consensus_signals': len(consensus_signals),
            'non_consensus_signals': len(non_consensus_signals),
            'consensus_details': [],
            'market_summary': self._generate_market_summary(signals),
            'performance_metrics': self._calculate_performance_metrics(signals)
        }
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¬Ø²Ø¦ÛŒØ§Øª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚
        for signal in consensus_signals:
            report['consensus_details'].append({
                'symbol': signal.get('SYMBOL'),
                'action': signal.get('ACTION'),
                'confidence': signal.get('CONFIDENCE'),
                'priority': signal.get('PRIORITY', 'MEDIUM'),
                'entry_zone': signal.get('ENTRY_ZONE'),
                'stop_loss': signal.get('STOP_LOSS'),
                'take_profit_1': signal.get('TAKE_PROFIT_1'),
                'take_profit_2': signal.get('TAKE_PROFIT_2'),
                'risk_reward': signal.get('RISK_REWARD_RATIO'),
                'expiration_hours': signal.get('EXPIRATION_H')
            })
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        report_file = "comprehensive_analysis_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)
        
        logging.info(f"ğŸ“‹ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø¯Ø± {report_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return report
    
    def _generate_market_summary(self, signals: List[Dict]) -> Dict:
        """ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ø¨Ø§Ø²Ø§Ø±"""
        buy_signals = [s for s in signals if s.get('ACTION') == 'BUY']
        sell_signals = [s for s in signals if s.get('ACTION') == 'SELL']
        
        avg_confidence = sum(float(s.get('CONFIDENCE', 0)) for s in signals) / len(signals) if signals else 0
        
        high_confidence_signals = [s for s in signals if float(s.get('CONFIDENCE', 0)) >= 8]
        medium_confidence_signals = [s for s in signals if 5 <= float(s.get('CONFIDENCE', 0)) < 8]
        
        return {
            'total_buy_signals': len(buy_signals),
            'total_sell_signals': len(sell_signals),
            'average_confidence': round(avg_confidence, 2),
            'high_confidence_count': len(high_confidence_signals),
            'medium_confidence_count': len(medium_confidence_signals),
            'market_bias': 'ØµØ¹ÙˆØ¯ÛŒ' if len(buy_signals) > len(sell_signals) else 'Ù†Ø²ÙˆÙ„ÛŒ' if len(sell_signals) > len(buy_signals) else 'Ø®Ù†Ø«ÛŒ',
            'signal_quality': 'Ø¹Ø§Ù„ÛŒ' if len(high_confidence_signals) >= 3 else 'Ø®ÙˆØ¨' if len(signals) >= 2 else 'Ø¶Ø¹ÛŒÙ'
        }
    
    def _calculate_performance_metrics(self, signals: List[Dict]) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        if not signals:
            return {'total_score': 0, 'quality_rating': 'Ø¶Ø¹ÛŒÙ'}
        
        confidence_sum = sum(float(s.get('CONFIDENCE', 0)) for s in signals)
        avg_confidence = confidence_sum / len(signals)
        
        consensus_count = sum(1 for s in signals if s.get('CONSENSUS', False))
        consensus_ratio = consensus_count / len(signals) if signals else 0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ
        total_score = (avg_confidence * 0.6) + (consensus_ratio * 40 * 0.4)  # Ù…Ù‚ÛŒØ§Ø³ 0-100
        
        if total_score >= 80:
            quality_rating = 'Ø¹Ø§Ù„ÛŒ'
        elif total_score >= 60:
            quality_rating = 'Ø®ÙˆØ¨'
        elif total_score >= 40:
            quality_rating = 'Ù…ØªÙˆØ³Ø·'
        else:
            quality_rating = 'Ø¶Ø¹ÛŒÙ'
        
        return {
            'total_score': round(total_score, 1),
            'quality_rating': quality_rating,
            'average_confidence': round(avg_confidence, 2),
            'consensus_ratio': round(consensus_ratio, 2),
            'signal_diversity': len(set(s.get('SYMBOL') for s in signals))
        }

# =================================================================================
# --- ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ GitHub Actions ---
# =================================================================================

async def github_actions_main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ GitHub Actions"""
    logging.info("ğŸ¯ Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ ÙØ§Ø±Ú©Ø³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ (GitHub Actions Optimized v2.1)")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†
    import argparse
    parser = argparse.ArgumentParser(description='Ø³ÛŒØ³ØªÙ… ØªØ­Ù„ÛŒÙ„ ÙØ§Ø±Ú©Ø³ Ø¨Ø§ AI ØªØ±Ú©ÛŒØ¨ÛŒ - GitHub Actions')
    parser.add_argument("--pair", type=str, help="ØªØ­Ù„ÛŒÙ„ Ø¬ÙØª Ø§Ø±Ø² Ù…Ø´Ø®Øµ (Ù…Ø«Ø§Ù„: EUR/USD)")
    parser.add_argument("--all", action="store_true", help="ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ù‡ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶")
    parser.add_argument("--pairs", type=str, help="ØªØ­Ù„ÛŒÙ„ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ (Ø¬Ø¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø§ Ú©Ø§Ù…Ø§)")
    parser.add_argument("--consensus-only", action="store_true", help="ÙÙ‚Ø· Ù†Ù…Ø§ÛŒØ´ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚")
    
    args = parser.parse_args()

    # ØªØ¹ÛŒÛŒÙ† Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
    if args.pair:
        pairs_to_analyze = [args.pair]
    elif args.pairs:
        pairs_to_analyze = [p.strip() for p in args.pairs.split(',')]
    elif args.all:
        pairs_to_analyze = CURRENCY_PAIRS_TO_ANALYZE
    else:
        # ØªØ­Ù„ÛŒÙ„ Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        pairs_to_analyze = CURRENCY_PAIRS_TO_ANALYZE[:6]
        logging.info(f"Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² 6 Ø¬ÙØª Ø§Ø±Ø² Ø§ØµÙ„ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶")

    logging.info(f"ğŸ” Ø¬ÙØª Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„: {', '.join(pairs_to_analyze)}")
    logging.info(f"ğŸ¤– Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI ÙØ¹Ø§Ù„: Gemini {GEMINI_MODEL} + Cloudflare {IMPROVED_CLOUDFLARE_MODELS[0]}")
    
    # Ø§ÛŒØ¬Ø§Ø¯ ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡
    analyzer = AdvancedForexAnalyzer()
    
    # Ø²Ù…Ø§Ù†â€ŒØ³Ù†Ø¬ Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø¹Ù…Ù„Ú©Ø±Ø¯
    start_time = time.time()
    signals = await analyzer.analyze_all_pairs(pairs_to_analyze)
    analysis_time = time.time() - start_time
    
    # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹
    if signals:
        report = analyzer.generate_comprehensive_report(signals)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
        consensus_signals = [s for s in signals if s.get('CONSENSUS', False)]
        
        logging.info("=" * 60)
        logging.info("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ ØªØ­Ù„ÛŒÙ„:")
        logging.info("=" * 60)
        logging.info(f"   â±ï¸  Ø²Ù…Ø§Ù† ØªØ­Ù„ÛŒÙ„: {analysis_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
        logging.info(f"   ğŸ“ˆ Ú©Ù„ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§: {len(signals)}")
        logging.info(f"   âœ… Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚: {len(consensus_signals)}")
        logging.info(f"   ğŸ¯ Ú©ÛŒÙÛŒØª Ú©Ù„ÛŒ: {report['performance_metrics']['quality_rating']}")
        logging.info(f"   ğŸ“Š ØªÙ…Ø§ÛŒÙ„ Ø¨Ø§Ø²Ø§Ø±: {report['market_summary']['market_bias']}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚
        if consensus_signals:
            logging.info("ğŸ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªÙˆØ§ÙÙ‚ Ø¨Ø§Ù„Ø§ (Ø§ÙˆÙ„ÙˆÛŒØª Ù…Ø¹Ø§Ù…Ù„Ù‡):")
            for signal in consensus_signals:
                action_icon = "ğŸŸ¢" if signal['ACTION'] == 'BUY' else "ğŸ”´"
                priority_icon = "ğŸ”¥" if signal.get('PRIORITY') == 'HIGH' else "âš¡" if signal.get('PRIORITY') == 'MEDIUM' else "ğŸ’¡"
                logging.info(f"   {action_icon} {priority_icon} {signal['SYMBOL']}: {signal['ACTION']} (Ø§Ø¹ØªÙ…Ø§Ø¯: {signal['CONFIDENCE']}/10)")
                
        # Ù†Ù…Ø§ÛŒØ´ Ø³Ø§ÛŒØ± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
        other_signals = [s for s in signals if not s.get('CONSENSUS', False)]
        if other_signals and not args.consensus_only:
            logging.info("ğŸ“‹ Ø³Ø§ÛŒØ± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ (Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ£ÛŒÛŒØ¯ Ø¨ÛŒØ´ØªØ±):")
            for signal in other_signals[:3]:  # Ù†Ù…Ø§ÛŒØ´ ÙÙ‚Ø· Û³ ØªØ§ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
                action_icon = "ğŸŸ¢" if signal['ACTION'] == 'BUY' else "ğŸ”´"
                logging.info(f"   {action_icon} {signal['SYMBOL']}: {signal['ACTION']} (Ø§Ø¹ØªÙ…Ø§Ø¯: {signal['CONFIDENCE']}/10)")
            
        if args.consensus_only and not consensus_signals:
            logging.info("ğŸ” Ù‡ÛŒÚ† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø§ ØªÙˆØ§ÙÙ‚ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø§Ø¬Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯")
            
    else:
        logging.info("ğŸ” Ù‡ÛŒÚ† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒâ€ŒØ§ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø§Ø¬Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯")
        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ø®Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ consistency
        empty_report = {
            'timestamp': datetime.now(UTC).isoformat(),
            'total_analyzed_pairs': len(pairs_to_analyze),
            'total_signals': 0,
            'consensus_signals': 0,
            'non_consensus_signals': 0,
            'consensus_details': [],
            'market_summary': {'market_bias': 'Ø®Ù†Ø«ÛŒ', 'signal_quality': 'Ø¶Ø¹ÛŒÙ'},
            'performance_metrics': {'total_score': 0, 'quality_rating': 'Ø¶Ø¹ÛŒÙ'}
        }
        with open("comprehensive_analysis_report.json", 'w', encoding='utf-8') as f:
            json.dump(empty_report, f, indent=4, ensure_ascii=False)

    logging.info("ğŸ Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… - Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ GitHub Actions")

if __name__ == "__main__":
    # Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…
    asyncio.run(github_actions_main())
